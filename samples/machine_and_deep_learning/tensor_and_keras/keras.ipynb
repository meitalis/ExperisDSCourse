{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SepalLength  SepalWidth  PetalLength  PetalWidth\n",
      "0            6.4         2.8          5.6         2.2\n",
      "1            5.0         2.3          3.3         1.0\n",
      "2            4.9         2.5          4.5         1.7\n",
      "3            4.9         3.1          1.5         0.1\n",
      "4            5.7         3.8          1.7         0.3\n",
      "..           ...         ...          ...         ...\n",
      "115          5.5         2.6          4.4         1.2\n",
      "116          5.7         3.0          4.2         1.2\n",
      "117          4.4         2.9          1.4         0.2\n",
      "118          4.8         3.0          1.4         0.1\n",
      "119          5.5         2.4          3.7         1.0\n",
      "\n",
      "[120 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# create input function\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "CSV_COLUMN_NAMES = ['SepalLength','SepalWidth',\n",
    "                   'PetalLength','PetalWidth','Species']\n",
    "SPECIES = ['Setosa','Versocolor','Virginica']\n",
    "\n",
    "def load_data():\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1],TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1],TEST_URL)\n",
    "    \n",
    "    train = pd.read_csv(train_path,names=CSV_COLUMN_NAMES,header=0)\n",
    "    X_train, y_train = train,train.pop('Species')\n",
    "    \n",
    "    test = pd.read_csv(test_path,names=CSV_COLUMN_NAMES,header=0)\n",
    "    X_test, y_test = test,test.pop('Species')\n",
    "    \n",
    "    return (X_train,y_train), (X_test,y_test)\n",
    "\n",
    "(X_train,y_train), (X_test,y_test) = load_data()\n",
    "print (X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features,labels,batch_size):\n",
    "    features = tf.cast(features,tf.float32)    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features,labels))\n",
    "    \n",
    "    return dataset.shuffle(1000,seed=42).repeat().batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features,labels,batch_size):\n",
    "    features = tf.cast(features,tf.float32)   \n",
    "    \n",
    "    if labels is None:\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features,labels)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    \n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    layers = tf.keras.layers\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(params['n_hidden1'],activation=tf.nn.relu),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(params['n_hidden2'],activation=tf.nn.relu),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(params['n_classes'])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model(features,labels,mode,params):\n",
    "            \n",
    "        \"\"\"DNN with 2 hidden layers , and dropout of 0.1 probability\"\"\"\n",
    "        model = create_model(params)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            logits = model(features,training=False)\n",
    "        \n",
    "            predictions = {\n",
    "                'class_ids' : tf.argmax(logits,axis=1),\n",
    "                'probabilities': tf.nn.softmax(logits),\n",
    "                'logits':logits\n",
    "            }\n",
    "\n",
    "            return tf.estimator.EstimatorSpec(mode,predictions=predictions)\n",
    "    \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:      \n",
    "            logits = model(features,training=True)\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
    "                                                      logits=logits)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "            \n",
    "            train_op = optimizer.minimize(loss,\n",
    "                                     global_step = tf.train.get_global_step())\n",
    "            \n",
    "            accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions = tf.argmax(logits,axis=1))\n",
    "            tf.summary.scalar('accuracy',accuracy[1])\n",
    "        \n",
    "            return tf.estimator.EstimatorSpec(mode,loss=loss,train_op=train_op)\n",
    "            \n",
    "        if mode == tf.estimator.ModeKeys.EVAL:      \n",
    "            logits = model(features,training=False)\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,\n",
    "                                                      logits=logits)\n",
    "            \n",
    "            accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                      predictions = tf.argmax(logits,axis=1))\n",
    "            metrics = {'accuracy':accuracy}\n",
    "        \n",
    "            return tf.estimator.EstimatorSpec(mode,loss=loss,eval_metric_ops = metric)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'my_model', '_tf_random_seed': 42, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002376C364988>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "config = tf.estimator.RunConfig(tf_random_seed=42)\n",
    "\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "        'n_hidden1': 10,\n",
    "        'n_hidden2': 10,\n",
    "        'n_classes': 3\n",
    "    },\n",
    "    model_dir = 'my_model',\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into my_model\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8516513, step = 0\n",
      "INFO:tensorflow:global_step/sec: 572.982\n",
      "INFO:tensorflow:loss = 0.28377807, step = 100 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 1222.77\n",
      "INFO:tensorflow:loss = 0.2356748, step = 200 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1139.41\n",
      "INFO:tensorflow:loss = 0.100969724, step = 300 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1253.34\n",
      "INFO:tensorflow:loss = 0.20656008, step = 400 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1222.78\n",
      "INFO:tensorflow:loss = 0.17643635, step = 500 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1237.87\n",
      "INFO:tensorflow:loss = 0.24931479, step = 600 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1269.21\n",
      "INFO:tensorflow:loss = 0.16662039, step = 700 (0.079 sec)\n",
      "INFO:tensorflow:global_step/sec: 1193.06\n",
      "INFO:tensorflow:loss = 0.16066952, step = 800 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1237.83\n",
      "INFO:tensorflow:loss = 0.1474628, step = 900 (0.081 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into my_model\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21629253.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x2376c2a8a48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_step = 1000\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(X_train,y_train,batch_size),\n",
    "    steps=train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
