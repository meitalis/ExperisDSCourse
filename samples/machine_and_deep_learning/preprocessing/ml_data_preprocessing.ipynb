{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn.preprocessing** package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for learning alg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing usually involves: \n",
    "1. changing the representation of the data (i.e from categorial to numerical)\n",
    "2. imputation of missing data\n",
    "3. discretization\n",
    "4. feature scaling\n",
    "\n",
    "before the preprocessing it is better to separate the predictors and the labels. \n",
    "we dont necessarily want to apply the same transformations to both of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Categorial Attributes - LabelEncoder & OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one issue with this represenation is that ML alg. will assume that two nearby values are more similar than two distant values\n",
    "\n",
    "to fix this issue, a common solution is to create one binary attibute per category, called **one-hot encoding**\n",
    "i.e: one attribute equal to 1 when is \"blue\" (and 0 otherwise), another attribute equal to 1 when is \"green\" (and 0 otherwise) and so on...\n",
    "\n",
    "note that fit_transform() of OneHotEncoder expects a 2D array, so we first need to reshape out columns into 2D arrays\n",
    "the output is a Scipy *sparse matrix*, instead of numpy array.\n",
    "\n",
    "after onehot encoding we get a matrix with thousands of columms and the matrix is full of zeros except for one 1 per row\n",
    "so instead of a sparse matrix only stores the location of the nonzero elements\n",
    "\n",
    "many (not all) estimators accept such sparse matrices\n",
    "\n",
    "you can use sparse matricses mostly like a normal 2D arraym but if you really want to convert to a dense numpy array call the **toarray()** method or to call **OneHotEncoder(sparse=False)**\n",
    "\n",
    "if there is a possibility that the training data might have missing categorial features, it can often be better to specify\n",
    "**handle_unknown='ignore'**. no error will be raised but the resulting columns for this features will be all zeros\n",
    "\n",
    "@see housing for LabelEncoder + OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>make</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>Checrolet</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>BMW</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yellow</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    color       make  year\n",
       "0   green  Checrolet  2017\n",
       "1    blue        BMW  2015\n",
       "2  yellow      Lexus  2018"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    {'color':'green','make': 'Checrolet','year':2017},\n",
    "    {'color':'blue','make': 'BMW','year':2015},\n",
    "    {'color':'yellow','make': 'Lexus','year':2018}\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "df['color_encoded'] = encoder.fit_transform(df['color'])\n",
    "df['make_encoded'] = encoder.fit_transform(df['make'])\n",
    "\n",
    "df\n",
    "df['color_encoded'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit 0.19 version OneHotEncoding needed numerical value first (earlier we couldn't directly encode string type data to numerical using OneHotEncoding, so first we used to apply LabelEncoding first) and then we used to apply OneHotEncoding.\n",
    "\n",
    "But Now (0.22) , OneHotEncoding could directly work with String data types also, but here we need data to be of type -either a DATAFRAME or a 2D Array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder1 = OneHotEncoder()\n",
    "# 2 options to reshape:\n",
    "    #df['color'].values[:,np.newaxis]\n",
    "    #df['color'].values.reshape(-1,1)\n",
    "\n",
    "#for old version 0.19\n",
    "#color_1hot = encoder1.fit_transform(df['color_encoded'].values.reshape(-1,1))\n",
    "#make_1hot = encoder1.fit_transform(df['make_encoded'].values.reshape(-1,1))\n",
    "\n",
    "#color_1hot\n",
    "\n",
    "color_1hot = encoder1.fit_transform(df['color'].values.reshape(-1,1))\n",
    "make_1hot = encoder1.fit_transform(df['make'].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color_1hot.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1 = OneHotEncoder(sparse=False)\n",
    "# 2 options to reshape:\n",
    "    #df['color'].values[:,np.newaxis]\n",
    "    #df['color'].values.reshape(-1,1)\n",
    "\n",
    "color_1hot = encoder1.fit_transform(df['color'].values.reshape(-1,1))\n",
    "make_1hot = encoder1.fit_transform(df['make'].values.reshape(-1,1))\n",
    "\n",
    "color_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imputation of missing data - SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most ML alg. cannot work with missing features. thus we need ti first replace missing data with some appropriate fill value\n",
    "\n",
    "SimpleImputer strategy - mean,median,constant,most_frequent\n",
    "\n",
    "the advantage of using imputer instead of filling NA values with pandas is the you can use the same imputer to replace missing values in the test set, and also once the system goes live \n",
    "\n",
    "imputing does not always improve prediction, we need to check via cross validation. sometime dropping rows or using marker values is more effective\n",
    "\n",
    "@see housing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5, 0. , 3. ],\n",
       "       [3. , 7. , 9. ],\n",
       "       [3. , 5. , 2. ],\n",
       "       [4. , 6. , 6. ],\n",
       "       [8. , 8. , 1. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[4.5, 0. , 3. ],\n",
       "       [3. , 7. , 9. ],\n",
       "       [3. , 5. , 2. ],\n",
       "       [4. , 5. , 6. ],\n",
       "       [8. , 8. , 1. ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([['a'],\n",
       "       ['b'],\n",
       "       ['a'],\n",
       "       ['a']], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([['a'],\n",
       "       ['b'],\n",
       "       ['a'],\n",
       "       ['b']], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[nan,0,3],\n",
    "             [3,7,9],\n",
    "             [3,5,2],\n",
    "             [4,nan,6],\n",
    "             [8,8,1]])\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'median')\n",
    "X = imputer.fit_transform(X)\n",
    "display(X)\n",
    "\n",
    "X = np.array([[nan,0,3],\n",
    "             [3,7,9],\n",
    "             [3,5,2],\n",
    "             [4,nan,6],\n",
    "             [8,8,1]])\n",
    "\n",
    "imputer_mean = SimpleImputer(strategy = 'mean')\n",
    "X2 = imputer_mean.fit_transform(X)\n",
    "display(X2)\n",
    "\n",
    "data = np.array(['a','b','a',np.nan],dtype = object).reshape(-1,1)\n",
    "imputer = SimpleImputer(strategy = 'most_frequent')\n",
    "data = imputer.fit_transform(data)\n",
    "display(data)\n",
    "\n",
    "data = np.array(['a','b','a',np.nan],dtype = object).reshape(-1,1)\n",
    "imputer = SimpleImputer(strategy = 'constant',fill_value='b')\n",
    "data = imputer.fit_transform(data)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization - KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discretization (binning or quantization) provides a way to partition continuous features into discrete value\n",
    "\n",
    "**KBinsDiscretizer strategy** : {'uniform', 'quantile', 'kmeans'}, (default='quantile')\n",
    "\n",
    "uniform :  all bins in each feature have identical widths\n",
    "    \n",
    "quantile : all bins in each feature have the same number of points.(the bins not always equal in this option)\n",
    "    \n",
    "kmeans : values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "\n",
    "\n",
    "**KBinsDiscretizer encode** : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')\n",
    "\n",
    "onehot : Encode the transformed result with one-hot encoding and return a sparse matrix. Ignored features are always\n",
    "        stacked to the right.\n",
    "        \n",
    "onehot-dense: Encode the transformed result with one-hot encoding and return a dense array. Ignored features are always\n",
    "        stacked to the right.\n",
    "        \n",
    "ordinal: Return the bin identifier encoded as an integer value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[-2,1,-1,-1],\n",
    "     [-1,2,-3,2],\n",
    "    [0,1,-2,0.5],\n",
    "    [1,4,-3,-0.5]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3,encode='onehot-dense')\n",
    "est.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n",
      "C:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:197: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  'decreasing the number of bins.' % jj)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([array([-2., -1.,  0.,  1.]), array([1., 2., 4.]),\n",
       "        array([-3., -2., -1.]), array([-1. , -0.5,  0.5,  2. ])],\n",
       "       dtype=object), array([[0., 0., 1., 0.],\n",
       "        [1., 1., 0., 2.],\n",
       "        [2., 0., 1., 2.],\n",
       "        [2., 1., 0., 1.]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[-2,1,-1,-1],\n",
    "     [-1,2,-3,2],\n",
    "     [0 ,1,-2,0.5],\n",
    "     [1 ,4,-3,-0.5]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3,encode='ordinal')\n",
    "x_trasformed = est.fit_transform(X)\n",
    "est.bin_edges_,x_trasformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([array([-2., -1.,  0.,  1.]), array([1., 2., 3., 4.]),\n",
       "        array([-3.        , -2.33333333, -1.66666667, -1.        ]),\n",
       "        array([-1.,  0.,  1.,  2.])], dtype=object), array([[0., 0., 2., 0.],\n",
       "        [1., 1., 0., 2.],\n",
       "        [2., 0., 1., 1.],\n",
       "        [2., 2., 0., 0.]]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[-2,1,-1,-1],\n",
    "     [-1,2,-3,2],\n",
    "     [0 ,1,-2,0.5],\n",
    "     [1 ,4,-3,-0.5]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3,encode='ordinal',strategy='uniform')\n",
    "x_trasformed = est.fit_transform(X)\n",
    "est.bin_edges_,x_trasformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([array([1.        , 2.33333333, 3.66666667, 5.        ])],\n",
       "       dtype=object), array([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1],[2],[3],[4],[5]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3,encode='ordinal',strategy='uniform')\n",
    "x_trasformed = est.fit_transform(X)\n",
    "est.bin_edges_,x_trasformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([array([1.        , 2.66666667, 4.33333333, 8.        ])],\n",
       "       dtype=object), array([[0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [2.]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1],[2],[3],[4],[5],[8]]\n",
    "\n",
    "est = KBinsDiscretizer(n_bins=3,encode='ordinal')\n",
    "x_trasformed = est.fit_transform(X)\n",
    "est.bin_edges_,x_trasformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling - MInMaxScaler and StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML alg. do not perform well when the input numerical attributes have very different scales\n",
    "there are 2 common ways to get all attributes to have the same scale:\n",
    "**min-max and standardization**\n",
    "\n",
    "**MInMaxScaler** values are shifted and rescaled so that they end up ranging from 0 to 1\n",
    "we do this by **(x - min value) / (max value - min value)**\n",
    "the **featue_range** hyper-parameter lets you change the range if you do not want 0-1\n",
    "\n",
    "**StandardScaler**   **(x - mean value) / variance**\n",
    "unlike minmax , standard does not bound values to a specific range. which may be a problem for some alg., which expect an input value in range of 0-1\n",
    "standardization is much less affected by outliers\n",
    "the resulting distribution in standardization has zero mean and unit variance\n",
    "\n",
    "@see housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [0.25, 0.25],\n",
       "       [0.5 , 0.5 ],\n",
       "       [1.  , 1.  ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[-1,2],\n",
    "        [-0.5,6],\n",
    "        [0,10],\n",
    "        [1,18]]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)\n",
    "\n",
    "#(-1 - (-1)/ (1- (-1)) = 0 / 2 = 0\n",
    "#(-0.5 - (-1)) / 2 = 0.5/2=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  ],\n",
       "       [1.25, 1.25],\n",
       "       [2.5 , 2.5 ],\n",
       "       [5.  , 5.  ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[-1,2],\n",
    "        [-0.5,6],\n",
    "        [0,10],\n",
    "        [1,18]]\n",
    "scaler = MinMaxScaler(feature_range=[0,5])\n",
    "scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18321596, -1.18321596],\n",
       "       [-0.50709255, -0.50709255],\n",
       "       [ 0.16903085,  0.16903085],\n",
       "       [ 1.52127766,  1.52127766]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[-1,2],\n",
    "        [-0.5,6],\n",
    "        [0,10],\n",
    "        [1,18]]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Column Transformer\n",
    "\n",
    "applies transformers to columns of an array or pandas DataFrame\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space\n",
    "\n",
    "This is a particularly handy for datasets that contain heterogeneous data types, since we may want to scale the numeric features and one hot encode the categorial ones\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list\n",
    "\n",
    "Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrixm unless specified in the **passthrough** keyword\n",
    "\n",
    "Those columns specified with passthrough are added at the right to the output of the transformers\n",
    "\n",
    "in classifier, the **score()** method returns the mean accuracy on the given test data\n",
    "\n",
    "@see housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., nan,  2.],\n",
       "       [ 1.,  1.,  0.,  0., nan]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  0.,  2.],\n",
       "       [ 1.,  0.,  0.,  2.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ct = ColumnTransformer([('scaler',StandardScaler(),[0,1]),\n",
    "                       ('imputer',SimpleImputer(),slice(3,5))])\n",
    "\n",
    "\n",
    "#a scaling is applied for the 2 first elements and an imputer is applied for the 2 last elements of each row\n",
    "X = np.array([[0,1,1,np.nan,2],\n",
    "        [1,1,0,0,np.nan]],dtype=float)\n",
    "display(X)\n",
    "\n",
    "X = ct.fit_transform(X)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832\n"
     ]
    }
   ],
   "source": [
    "#get the titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "display(titanic.head())\n",
    "\n",
    "#create the preprocessing pipeline for the numerical data\n",
    "numeric_features = ['age','fare']\n",
    "numeric_transformer = Pipeline([('imputer',SimpleImputer(strategy='median')),\n",
    "                               ('scaler',StandardScaler())])\n",
    "\n",
    "#create the preprocessing pipeline for the categorial data\n",
    "category_features = ['embarked','sex','pclass']\n",
    "category_transformer = Pipeline([('imputer',SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "                               ('onehot',OneHotEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num',numeric_transformer,numeric_features),\n",
    "    ('cat',category_transformer,category_features)\n",
    "])\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('pre',preprocessor),\n",
    "    ('cls',RandomForestClassifier(n_estimators=20))\n",
    "])\n",
    "\n",
    "X = titanic.drop('survived',axis=1)\n",
    "y = titanic['survived']\n",
    "\n",
    "X_train,X_test,y_train ,y_test = train_test_split(X,y,test_size=0.2)\n",
    "X_train,X_test ,y_train ,y_test\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "print(np.round(clf.score(X_test,y_test),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although scikit learn provides many useful transformers m you will need ti write your own for tasks such as custom cleanup operation or combining specific attributes\n",
    "\n",
    "you will want your transformer to work seamlessly with scikit learn functionalities\n",
    "you need to implements three methods:\n",
    "fit() - return itself\n",
    "transform()\n",
    "fit_transform()\n",
    "\n",
    "you can get it for free by adding **TransormerMixin** as a base class\n",
    "if you add **BaseEstimator** as a base class you will get 2 extra methods **get_params() and set_params()** that are useful for hyperparameter tuning\n",
    "\n",
    "@see housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
