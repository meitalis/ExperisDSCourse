{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-427d214314ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write KMeans Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKMeans(BaseEstimator):\n",
    "     def __init__(self, n_clusters=8, n_init=10,max_iter=300, tol=1e-4,random_state=None ):\n",
    "\n",
    "        self.n_clusters = n_clusters #n_samples should be >= n_clusters\n",
    "        self.max_iter = max_iter #Number of iterations should be a positive number\n",
    "        self.tol = tol\n",
    "        self.n_init = n_init #must be bigger than zero\n",
    "        self.random_state = random_state\n",
    "     \n",
    "     def fit(self, X, y=None, sample_weight=None):\n",
    "        \n",
    "               \n",
    "        # Validate init array\n",
    "        init = self.init\n",
    "        if hasattr(init, '__array__'):\n",
    "            init = check_array(init, dtype=X.dtype.type, copy=True)\n",
    "            _validate_center_shape(X, self.n_clusters, init)\n",
    "\n",
    "            if n_init != 1:\n",
    "                warnings.warn(\n",
    "                    'Explicit initial center position passed: '\n",
    "                    'performing only one init in k-means instead of n_init=%d'\n",
    "                    % n_init, RuntimeWarning, stacklevel=2)\n",
    "                n_init = 1\n",
    "\n",
    "        # subtract of mean of x for more accurate distance computations\n",
    "        if not sp.issparse(X):\n",
    "            X_mean = X.mean(axis=0)\n",
    "            # The copy was already done above\n",
    "            X -= X_mean\n",
    "\n",
    "            if hasattr(init, '__array__'):\n",
    "                init -= X_mean\n",
    "\n",
    "        # precompute squared norms of data points\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "        best_labels, best_inertia, best_centers = None, None, None\n",
    "        algorithm = self.algorithm\n",
    "        if self.n_clusters == 1:\n",
    "            # elkan doesn't make sense for a single cluster, full will produce\n",
    "            # the right result.\n",
    "            algorithm = \"full\"\n",
    "        if algorithm == \"auto\":\n",
    "            algorithm = \"full\" if sp.issparse(X) else 'elkan'\n",
    "        if algorithm == \"full\":\n",
    "            kmeans_single = _kmeans_single_lloyd\n",
    "        elif algorithm == \"elkan\":\n",
    "            kmeans_single = _kmeans_single_elkan\n",
    "        else:\n",
    "            raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n",
    "                             \" %s\" % str(algorithm))\n",
    "\n",
    "        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n",
    "        if effective_n_jobs(self.n_jobs) == 1:\n",
    "            # For a single thread, less memory is needed if we just store one\n",
    "            # set of the best results (as opposed to one set per run per\n",
    "            # thread).\n",
    "            for seed in seeds:\n",
    "                # run a k-means once\n",
    "                labels, inertia, centers, n_iter_ = kmeans_single(\n",
    "                    X, sample_weight, self.n_clusters,\n",
    "                    max_iter=self.max_iter, init=init, verbose=self.verbose,\n",
    "                    precompute_distances=precompute_distances, tol=tol,\n",
    "                    x_squared_norms=x_squared_norms, random_state=seed)\n",
    "                # determine if these results are the best so far\n",
    "                if best_inertia is None or inertia < best_inertia:\n",
    "                    best_labels = labels.copy()\n",
    "                    best_centers = centers.copy()\n",
    "                    best_inertia = inertia\n",
    "                    best_n_iter = n_iter_\n",
    "        else:\n",
    "            # parallelisation of k-means runs\n",
    "            results = Parallel(n_jobs=self.n_jobs, verbose=0)(\n",
    "                delayed(kmeans_single)(\n",
    "                    X, sample_weight, self.n_clusters,\n",
    "                    max_iter=self.max_iter, init=init,\n",
    "                    verbose=self.verbose, tol=tol,\n",
    "                    precompute_distances=precompute_distances,\n",
    "                    x_squared_norms=x_squared_norms,\n",
    "                    # Change seed to ensure variety\n",
    "                    random_state=seed\n",
    "                )\n",
    "                for seed in seeds)\n",
    "            # Get results with the lowest inertia\n",
    "            labels, inertia, centers, n_iters = zip(*results)\n",
    "            best = np.argmin(inertia)\n",
    "            best_labels = labels[best]\n",
    "            best_inertia = inertia[best]\n",
    "            best_centers = centers[best]\n",
    "            best_n_iter = n_iters[best]\n",
    "\n",
    "        if not sp.issparse(X):\n",
    "            if not self.copy_x:\n",
    "                X += X_mean\n",
    "            best_centers += X_mean\n",
    "\n",
    "        distinct_clusters = len(set(best_labels))\n",
    "        if distinct_clusters < self.n_clusters:\n",
    "            warnings.warn(\n",
    "                \"Number of distinct clusters ({}) found smaller than \"\n",
    "                \"n_clusters ({}). Possibly due to duplicate points \"\n",
    "                \"in X.\".format(distinct_clusters, self.n_clusters),\n",
    "                ConvergenceWarning, stacklevel=2\n",
    "            )\n",
    "\n",
    "        self.cluster_centers_ = best_centers\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        self.n_iter_ = best_n_iter\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Initialization heuristic\n",
    "\n",
    "\n",
    "def _k_init(X, n_clusters, x_squared_norms, random_state, n_local_trials=None):\n",
    "    \"\"\"Init n_clusters seeds according to k-means++\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array or sparse matrix, shape (n_samples, n_features)\n",
    "        The data to pick seeds for. To avoid memory copy, the input data\n",
    "        should be double precision (dtype=np.float64).\n",
    "    n_clusters : integer\n",
    "        The number of seeds to choose\n",
    "    x_squared_norms : array, shape (n_samples,)\n",
    "        Squared Euclidean norm of each data point.\n",
    "    random_state : int, RandomState instance\n",
    "        The generator used to initialize the centers. Use an int to make the\n",
    "        randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    n_local_trials : integer, optional\n",
    "        The number of seeding trials for each center (except the first),\n",
    "        of which the one reducing inertia the most is greedily chosen.\n",
    "        Set to None to make the number of trials depend logarithmically\n",
    "        on the number of seeds (2+log(k)); this is the default.\n",
    "    Notes\n",
    "    -----\n",
    "    Selects initial cluster centers for k-mean clustering in a smart way\n",
    "    to speed up convergence. see: Arthur, D. and Vassilvitskii, S.\n",
    "    \"k-means++: the advantages of careful seeding\". ACM-SIAM symposium\n",
    "    on Discrete algorithms. 2007\n",
    "    Version ported from http://www.stanford.edu/~darthur/kMeansppTest.zip,\n",
    "    which is the implementation used in the aforementioned paper.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    centers = np.empty((n_clusters, n_features), dtype=X.dtype)\n",
    "\n",
    "    assert x_squared_norms is not None, 'x_squared_norms None in _k_init'\n",
    "\n",
    "    # Set the number of local seeding trials if none is given\n",
    "    if n_local_trials is None:\n",
    "        # This is what Arthur/Vassilvitskii tried, but did not report\n",
    "        # specific results for other than mentioning in the conclusion\n",
    "        # that it helped.\n",
    "        n_local_trials = 2 + int(np.log(n_clusters))\n",
    "\n",
    "    # Pick first center randomly\n",
    "    center_id = random_state.randint(n_samples)\n",
    "    if sp.issparse(X):\n",
    "        centers[0] = X[center_id].toarray()\n",
    "    else:\n",
    "        centers[0] = X[center_id]\n",
    "\n",
    "    # Initialize list of closest distances and calculate current potential\n",
    "    closest_dist_sq = euclidean_distances(\n",
    "        centers[0, np.newaxis], X, Y_norm_squared=x_squared_norms,\n",
    "        squared=True)\n",
    "    current_pot = closest_dist_sq.sum()\n",
    "\n",
    "    # Pick the remaining n_clusters-1 points\n",
    "    for c in range(1, n_clusters):\n",
    "        # Choose center candidates by sampling with probability proportional\n",
    "        # to the squared distance to the closest existing center\n",
    "        rand_vals = random_state.random_sample(n_local_trials) * current_pot\n",
    "        candidate_ids = np.searchsorted(stable_cumsum(closest_dist_sq),\n",
    "                                        rand_vals)\n",
    "        # XXX: numerical imprecision can result in a candidate_id out of range\n",
    "        np.clip(candidate_ids, None, closest_dist_sq.size - 1,\n",
    "                out=candidate_ids)\n",
    "\n",
    "        # Compute distances to center candidates\n",
    "        distance_to_candidates = euclidean_distances(\n",
    "            X[candidate_ids], X, Y_norm_squared=x_squared_norms, squared=True)\n",
    "\n",
    "        # update closest distances squared and potential for each candidate\n",
    "        np.minimum(closest_dist_sq, distance_to_candidates,\n",
    "                   out=distance_to_candidates)\n",
    "        candidates_pot = distance_to_candidates.sum(axis=1)\n",
    "\n",
    "        # Decide which candidate is the best\n",
    "        best_candidate = np.argmin(candidates_pot)\n",
    "        current_pot = candidates_pot[best_candidate]\n",
    "        closest_dist_sq = distance_to_candidates[best_candidate]\n",
    "        best_candidate = candidate_ids[best_candidate]\n",
    "\n",
    "        # Permanently add best center candidate found in local tries\n",
    "        if sp.issparse(X):\n",
    "            centers[c] = X[best_candidate].toarray()\n",
    "        else:\n",
    "            centers[c] = X[best_candidate]\n",
    "\n",
    "    return centers\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# K-means batch estimation by EM (expectation maximization)\n",
    "\n",
    "def _validate_center_shape(X, n_centers, centers):\n",
    "    \"\"\"Check if centers is compatible with X and n_centers\"\"\"\n",
    "    if len(centers) != n_centers:\n",
    "        raise ValueError('The shape of the initial centers (%s) '\n",
    "                         'does not match the number of clusters %i'\n",
    "                         % (centers.shape, n_centers))\n",
    "    if centers.shape[1] != X.shape[1]:\n",
    "        raise ValueError(\n",
    "            \"The number of features of the initial centers %s \"\n",
    "            \"does not match the number of features of the data %s.\"\n",
    "            % (centers.shape[1], X.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "def _check_normalize_sample_weight(sample_weight, X):\n",
    "    \"\"\"Set sample_weight if None, and check for correct dtype\"\"\"\n",
    "\n",
    "    sample_weight_was_none = sample_weight is None\n",
    "\n",
    "    sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
    "    if not sample_weight_was_none:\n",
    "        # normalize the weights to sum up to n_samples\n",
    "        # an array of 1 (i.e. samples_weight is None) is already normalized\n",
    "        n_samples = len(sample_weight)\n",
    "        scale = n_samples / sample_weight.sum()\n",
    "        sample_weight *= scale\n",
    "    return sample_weight\n",
    "\n",
    "\n",
    "def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n",
    "            precompute_distances='auto', n_init=10, max_iter=300,\n",
    "            verbose=False, tol=1e-4, random_state=None, copy_x=True,\n",
    "            n_jobs=None, algorithm=\"auto\", return_n_iter=False):\n",
    "    \"\"\"K-means clustering algorithm.\n",
    "    Read more in the :ref:`User Guide <k_means>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "        The observations to cluster. It must be noted that the data\n",
    "        will be converted to C ordering, which will cause a memory copy\n",
    "        if the given data is not C-contiguous.\n",
    "    n_clusters : int\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    sample_weight : array-like, shape (n_samples,), optional\n",
    "        The weights for each observation in X. If None, all observations\n",
    "        are assigned equal weight (default: None)\n",
    "    init : {'k-means++', 'random', or ndarray, or a callable}, optional\n",
    "        Method for initialization, default to 'k-means++':\n",
    "        'k-means++' : selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence. See section\n",
    "        Notes in k_init for more details.\n",
    "        'random': choose k observations (rows) at random from data for\n",
    "        the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
    "        and gives the initial centers.\n",
    "        If a callable is passed, it should take arguments X, k and\n",
    "        and a random state and return an initialization.\n",
    "    precompute_distances : {'auto', True, False}\n",
    "        Precompute distances (faster but takes more memory).\n",
    "        'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
    "        million. This corresponds to about 100MB overhead per job using\n",
    "        double precision.\n",
    "        True : always precompute distances\n",
    "        False : never precompute distances\n",
    "    n_init : int, optional, default: 10\n",
    "        Number of time the k-means algorithm will be run with different\n",
    "        centroid seeds. The final results will be the best output of\n",
    "        n_init consecutive runs in terms of inertia.\n",
    "    max_iter : int, optional, default 300\n",
    "        Maximum number of iterations of the k-means algorithm to run.\n",
    "    verbose : boolean, optional\n",
    "        Verbosity mode.\n",
    "    tol : float, optional\n",
    "        The relative increment in the results before declaring convergence.\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for centroid initialization. Use\n",
    "        an int to make the randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    copy_x : bool, optional\n",
    "        When pre-computing distances it is more numerically accurate to center\n",
    "        the data first.  If copy_x is True (default), then the original data is\n",
    "        not modified, ensuring X is C-contiguous.  If False, the original data\n",
    "        is modified, and put back before the function returns, but small\n",
    "        numerical differences may be introduced by subtracting and then adding\n",
    "        the data mean, in this case it will also not ensure that data is\n",
    "        C-contiguous which may cause a significant slowdown.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        The number of jobs to use for the computation. This works by computing\n",
    "        each of the n_init runs in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "    algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n",
    "        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
    "        The \"elkan\" variation is more efficient by using the triangle\n",
    "        inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
    "        \"elkan\" for dense data and \"full\" for sparse data.\n",
    "    return_n_iter : bool, optional\n",
    "        Whether or not to return the number of iterations.\n",
    "    Returns\n",
    "    -------\n",
    "    centroid : float ndarray with shape (k, n_features)\n",
    "        Centroids found at the last iteration of k-means.\n",
    "    label : integer ndarray with shape (n_samples,)\n",
    "        label[i] is the code or index of the centroid the\n",
    "        i'th observation is closest to.\n",
    "    inertia : float\n",
    "        The final value of the inertia criterion (sum of squared distances to\n",
    "        the closest centroid for all observations in the training set).\n",
    "    best_n_iter : int\n",
    "        Number of iterations corresponding to the best results.\n",
    "        Returned only if `return_n_iter` is set to True.\n",
    "    \"\"\"\n",
    "\n",
    "    est = KMeans(\n",
    "        n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter,\n",
    "        verbose=verbose, precompute_distances=precompute_distances, tol=tol,\n",
    "        random_state=random_state, copy_x=copy_x, n_jobs=n_jobs,\n",
    "        algorithm=algorithm\n",
    "    ).fit(X, sample_weight=sample_weight)\n",
    "    if return_n_iter:\n",
    "        return est.cluster_centers_, est.labels_, est.inertia_, est.n_iter_\n",
    "    else:\n",
    "        return est.cluster_centers_, est.labels_, est.inertia_\n",
    "\n",
    "\n",
    "def _kmeans_single_elkan(X, sample_weight, n_clusters, max_iter=300,\n",
    "                         init='k-means++', verbose=False, x_squared_norms=None,\n",
    "                         random_state=None, tol=1e-4,\n",
    "                         precompute_distances=True):\n",
    "    if sp.issparse(X):\n",
    "        raise TypeError(\"algorithm='elkan' not supported for sparse input X\")\n",
    "    random_state = check_random_state(random_state)\n",
    "    if x_squared_norms is None:\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "    # init\n",
    "    centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n",
    "                              x_squared_norms=x_squared_norms)\n",
    "    centers = np.ascontiguousarray(centers)\n",
    "    if verbose:\n",
    "        print('Initialization complete')\n",
    "\n",
    "    checked_sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "    centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n",
    "                                            n_clusters, centers, tol=tol,\n",
    "                                            max_iter=max_iter, verbose=verbose)\n",
    "    if sample_weight is None:\n",
    "        inertia = np.sum((X - centers[labels]) ** 2, dtype=np.float64)\n",
    "    else:\n",
    "        sq_distances = np.sum((X - centers[labels]) ** 2, axis=1,\n",
    "                              dtype=np.float64) * checked_sample_weight\n",
    "        inertia = np.sum(sq_distances, dtype=np.float64)\n",
    "    return labels, inertia, centers, n_iter\n",
    "\n",
    "\n",
    "def _kmeans_single_lloyd(X, sample_weight, n_clusters, max_iter=300,\n",
    "                         init='k-means++', verbose=False, x_squared_norms=None,\n",
    "                         random_state=None, tol=1e-4,\n",
    "                         precompute_distances=True):\n",
    "    \"\"\"A single run of k-means, assumes preparation completed prior.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like of floats, shape (n_samples, n_features)\n",
    "        The observations to cluster.\n",
    "    n_clusters : int\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    sample_weight : array-like, shape (n_samples,)\n",
    "        The weights for each observation in X.\n",
    "    max_iter : int, optional, default 300\n",
    "        Maximum number of iterations of the k-means algorithm to run.\n",
    "    init : {'k-means++', 'random', or ndarray, or a callable}, optional\n",
    "        Method for initialization, default to 'k-means++':\n",
    "        'k-means++' : selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence. See section\n",
    "        Notes in k_init for more details.\n",
    "        'random': choose k observations (rows) at random from data for\n",
    "        the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (k, p) and gives\n",
    "        the initial centers.\n",
    "        If a callable is passed, it should take arguments X, k and\n",
    "        and a random state and return an initialization.\n",
    "    tol : float, optional\n",
    "        The relative increment in the results before declaring convergence.\n",
    "    verbose : boolean, optional\n",
    "        Verbosity mode\n",
    "    x_squared_norms : array\n",
    "        Precomputed x_squared_norms.\n",
    "    precompute_distances : boolean, default: True\n",
    "        Precompute distances (faster but takes more memory).\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for centroid initialization. Use\n",
    "        an int to make the randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    Returns\n",
    "    -------\n",
    "    centroid : float ndarray with shape (k, n_features)\n",
    "        Centroids found at the last iteration of k-means.\n",
    "    label : integer ndarray with shape (n_samples,)\n",
    "        label[i] is the code or index of the centroid the\n",
    "        i'th observation is closest to.\n",
    "    inertia : float\n",
    "        The final value of the inertia criterion (sum of squared distances to\n",
    "        the closest centroid for all observations in the training set).\n",
    "    n_iter : int\n",
    "        Number of iterations run.\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "\n",
    "    best_labels, best_inertia, best_centers = None, None, None\n",
    "    # init\n",
    "    centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n",
    "                              x_squared_norms=x_squared_norms)\n",
    "    if verbose:\n",
    "        print(\"Initialization complete\")\n",
    "\n",
    "    # Allocate memory to store the distances for each sample to its\n",
    "    # closer center for reallocation in case of ties\n",
    "    distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype)\n",
    "\n",
    "    # iterations\n",
    "    for i in range(max_iter):\n",
    "        centers_old = centers.copy()\n",
    "        # labels assignment is also called the E-step of EM\n",
    "        labels, inertia = \\\n",
    "            _labels_inertia(X, sample_weight, x_squared_norms, centers,\n",
    "                            precompute_distances=precompute_distances,\n",
    "                            distances=distances)\n",
    "\n",
    "        # computation of the means is also called the M-step of EM\n",
    "        if sp.issparse(X):\n",
    "            centers = _k_means._centers_sparse(X, sample_weight, labels,\n",
    "                                               n_clusters, distances)\n",
    "        else:\n",
    "            centers = _k_means._centers_dense(X, sample_weight, labels,\n",
    "                                              n_clusters, distances)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Iteration %2d, inertia %.3f\" % (i, inertia))\n",
    "\n",
    "        if best_inertia is None or inertia < best_inertia:\n",
    "            best_labels = labels.copy()\n",
    "            best_centers = centers.copy()\n",
    "            best_inertia = inertia\n",
    "\n",
    "        center_shift_total = squared_norm(centers_old - centers)\n",
    "        if center_shift_total <= tol:\n",
    "            if verbose:\n",
    "                print(\"Converged at iteration %d: \"\n",
    "                      \"center shift %e within tolerance %e\"\n",
    "                      % (i, center_shift_total, tol))\n",
    "            break\n",
    "\n",
    "    if center_shift_total > 0:\n",
    "        # rerun E-step in case of non-convergence so that predicted labels\n",
    "        # match cluster centers\n",
    "        best_labels, best_inertia = \\\n",
    "            _labels_inertia(X, sample_weight, x_squared_norms, best_centers,\n",
    "                            precompute_distances=precompute_distances,\n",
    "                            distances=distances)\n",
    "\n",
    "    return best_labels, best_inertia, best_centers, i + 1\n",
    "\n",
    "\n",
    "def _labels_inertia_precompute_dense(X, sample_weight, x_squared_norms,\n",
    "                                     centers, distances):\n",
    "    \"\"\"Compute labels and inertia using a full distance matrix.\n",
    "    This will overwrite the 'distances' array in-place.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (n_sample, n_features)\n",
    "        Input data.\n",
    "    sample_weight : array-like, shape (n_samples,)\n",
    "        The weights for each observation in X.\n",
    "    x_squared_norms : numpy array, shape (n_samples,)\n",
    "        Precomputed squared norms of X.\n",
    "    centers : numpy array, shape (n_clusters, n_features)\n",
    "        Cluster centers which data is assigned to.\n",
    "    distances : numpy array, shape (n_samples,)\n",
    "        Pre-allocated array in which distances are stored.\n",
    "    Returns\n",
    "    -------\n",
    "    labels : numpy array, dtype=np.int, shape (n_samples,)\n",
    "        Indices of clusters that samples are assigned to.\n",
    "    inertia : float\n",
    "        Sum of squared distances of samples to their closest cluster center.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Breakup nearest neighbor distance computation into batches to prevent\n",
    "    # memory blowup in the case of a large number of samples and clusters.\n",
    "    # TODO: Once PR #7383 is merged use check_inputs=False in metric_kwargs.\n",
    "    labels, mindist = pairwise_distances_argmin_min(\n",
    "        X=X, Y=centers, metric='euclidean', metric_kwargs={'squared': True})\n",
    "    # cython k-means code assumes int32 inputs\n",
    "    labels = labels.astype(np.int32, copy=False)\n",
    "    if n_samples == distances.shape[0]:\n",
    "        # distances will be changed in-place\n",
    "        distances[:] = mindist\n",
    "    inertia = (mindist * sample_weight).sum()\n",
    "    return labels, inertia\n",
    "\n",
    "\n",
    "def _labels_inertia(X, sample_weight, x_squared_norms, centers,\n",
    "                    precompute_distances=True, distances=None):\n",
    "    \"\"\"E step of the K-means EM algorithm.\n",
    "    Compute the labels and the inertia of the given samples and centers.\n",
    "    This will compute the distances in-place.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : float64 array-like or CSR sparse matrix, shape (n_samples, n_features)\n",
    "        The input samples to assign to the labels.\n",
    "    sample_weight : array-like, shape (n_samples,)\n",
    "        The weights for each observation in X.\n",
    "    x_squared_norms : array, shape (n_samples,)\n",
    "        Precomputed squared euclidean norm of each data point, to speed up\n",
    "        computations.\n",
    "    centers : float array, shape (k, n_features)\n",
    "        The cluster centers.\n",
    "    precompute_distances : boolean, default: True\n",
    "        Precompute distances (faster but takes more memory).\n",
    "    distances : float array, shape (n_samples,)\n",
    "        Pre-allocated array to be filled in with each sample's distance\n",
    "        to the closest center.\n",
    "    Returns\n",
    "    -------\n",
    "    labels : int array of shape(n)\n",
    "        The resulting assignment\n",
    "    inertia : float\n",
    "        Sum of squared distances of samples to their closest cluster center.\n",
    "    \"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "    # set the default value of centers to -1 to be able to detect any anomaly\n",
    "    # easily\n",
    "    labels = np.full(n_samples, -1, np.int32)\n",
    "    if distances is None:\n",
    "        distances = np.zeros(shape=(0,), dtype=X.dtype)\n",
    "    # distances will be changed in-place\n",
    "    if sp.issparse(X):\n",
    "        inertia = _k_means._assign_labels_csr(\n",
    "            X, sample_weight, x_squared_norms, centers, labels,\n",
    "            distances=distances)\n",
    "    else:\n",
    "        if precompute_distances:\n",
    "            return _labels_inertia_precompute_dense(X, sample_weight,\n",
    "                                                    x_squared_norms, centers,\n",
    "                                                    distances)\n",
    "        inertia = _k_means._assign_labels_array(\n",
    "            X, sample_weight, x_squared_norms, centers, labels,\n",
    "            distances=distances)\n",
    "    return labels, inertia\n",
    "\n",
    "\n",
    "def _init_centroids(X, k, init, random_state=None, x_squared_norms=None,\n",
    "                    init_size=None):\n",
    "    \"\"\"Compute the initial centroids\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "    k : int\n",
    "        number of centroids\n",
    "    init : {'k-means++', 'random' or ndarray or callable} optional\n",
    "        Method for initialization\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for centroid initialization. Use\n",
    "        an int to make the randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    x_squared_norms : array, shape (n_samples,), optional\n",
    "        Squared euclidean norm of each data point. Pass it if you have it at\n",
    "        hands already to avoid it being recomputed here. Default: None\n",
    "    init_size : int, optional\n",
    "        Number of samples to randomly sample for speeding up the\n",
    "        initialization (sometimes at the expense of accuracy): the\n",
    "        only algorithm is initialized by running a batch KMeans on a\n",
    "        random subset of the data. This needs to be larger than k.\n",
    "    Returns\n",
    "    -------\n",
    "    centers : array, shape(k, n_features)\n",
    "    \"\"\"\n",
    "    random_state = check_random_state(random_state)\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    if x_squared_norms is None:\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "    if init_size is not None and init_size < n_samples:\n",
    "        if init_size < k:\n",
    "            warnings.warn(\n",
    "                \"init_size=%d should be larger than k=%d. \"\n",
    "                \"Setting it to 3*k\" % (init_size, k),\n",
    "                RuntimeWarning, stacklevel=2)\n",
    "            init_size = 3 * k\n",
    "        init_indices = random_state.randint(0, n_samples, init_size)\n",
    "        X = X[init_indices]\n",
    "        x_squared_norms = x_squared_norms[init_indices]\n",
    "        n_samples = X.shape[0]\n",
    "    elif n_samples < k:\n",
    "        raise ValueError(\n",
    "            \"n_samples=%d should be larger than k=%d\" % (n_samples, k))\n",
    "\n",
    "    if isinstance(init, str) and init == 'k-means++':\n",
    "        centers = _k_init(X, k, random_state=random_state,\n",
    "                          x_squared_norms=x_squared_norms)\n",
    "    elif isinstance(init, str) and init == 'random':\n",
    "        seeds = random_state.permutation(n_samples)[:k]\n",
    "        centers = X[seeds]\n",
    "    elif hasattr(init, '__array__'):\n",
    "        # ensure that the centers have the same dtype as X\n",
    "        # this is a requirement of fused types of cython\n",
    "        centers = np.array(init, dtype=X.dtype)\n",
    "    elif callable(init):\n",
    "        centers = init(X, k, random_state=random_state)\n",
    "        centers = np.asarray(centers, dtype=X.dtype)\n",
    "    else:\n",
    "        raise ValueError(\"the init parameter for the k-means should \"\n",
    "                         \"be 'k-means++' or 'random' or an ndarray, \"\n",
    "                         \"'%s' (type '%s') was passed.\" % (init, type(init)))\n",
    "\n",
    "    if sp.issparse(centers):\n",
    "        centers = centers.toarray()\n",
    "\n",
    "    _validate_center_shape(X, k, centers)\n",
    "    return centers\n",
    "\n",
    "\n",
    "class KMeans(TransformerMixin, ClusterMixin, BaseEstimator):\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    def _check_test_data(self, X):\n",
    "        X = check_array(X, accept_sparse='csr', dtype=FLOAT_DTYPES)\n",
    "        n_samples, n_features = X.shape\n",
    "        expected_n_features = self.cluster_centers_.shape[1]\n",
    "        if not n_features == expected_n_features:\n",
    "            raise ValueError(\"Incorrect number of features. \"\n",
    "                             \"Got %d features, expected %d\" % (\n",
    "                                 n_features, expected_n_features))\n",
    "\n",
    "        return X\n",
    "\n",
    "   \n",
    "    def fit_predict(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Compute cluster centers and predict cluster index for each sample.\n",
    "        Convenience method; equivalent to calling fit(X) followed by\n",
    "        predict(X).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data to transform.\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape [n_samples,]\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        return self.fit(X, sample_weight=sample_weight).labels_\n",
    "\n",
    "    def fit_transform(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Compute clustering and transform X to cluster-distance space.\n",
    "        Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data to transform.\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape [n_samples, k]\n",
    "            X transformed in the new space.\n",
    "        \"\"\"\n",
    "        # Currently, this just skips a copy of the data if it is not in\n",
    "        # np.array or CSR format already.\n",
    "        # XXX This skips _check_test_data, which may change the dtype;\n",
    "        # we should refactor the input validation.\n",
    "        return self.fit(X, sample_weight=sample_weight)._transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X to a cluster-distance space.\n",
    "        In the new space, each dimension is the distance to the cluster\n",
    "        centers.  Note that even if X is sparse, the array returned by\n",
    "        `transform` will typically be dense.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data to transform.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape [n_samples, k]\n",
    "            X transformed in the new space.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._check_test_data(X)\n",
    "        return self._transform(X)\n",
    "\n",
    "    def _transform(self, X):\n",
    "        \"\"\"guts of transform method; no input validation\"\"\"\n",
    "        return euclidean_distances(X, self.cluster_centers_)\n",
    "\n",
    "    def predict(self, X, sample_weight=None):\n",
    "        \"\"\"Predict the closest cluster each sample in X belongs to.\n",
    "        In the vector quantization literature, `cluster_centers_` is called\n",
    "        the code book and each value returned by `predict` is the index of\n",
    "        the closest code in the code book.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data to predict.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape [n_samples,]\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._check_test_data(X)\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "        return _labels_inertia(X, sample_weight, x_squared_norms,\n",
    "                               self.cluster_centers_)[0]\n",
    "\n",
    "    def score(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Opposite of the value of X on the K-means objective.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data.\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Opposite of the value of X on the K-means objective.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._check_test_data(X)\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "        return -_labels_inertia(X, sample_weight, x_squared_norms,\n",
    "                                self.cluster_centers_)[1]\n",
    "\n",
    "\n",
    "def _mini_batch_step(X, sample_weight, x_squared_norms, centers, weight_sums,\n",
    "                     old_center_buffer, compute_squared_diff,\n",
    "                     distances, random_reassign=False,\n",
    "                     random_state=None, reassignment_ratio=.01,\n",
    "                     verbose=False):\n",
    "    \"\"\"Incremental update of the centers for the Minibatch K-Means algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The original data array.\n",
    "    sample_weight : array-like, shape (n_samples,)\n",
    "        The weights for each observation in X.\n",
    "    x_squared_norms : array, shape (n_samples,)\n",
    "        Squared euclidean norm of each data point.\n",
    "    centers : array, shape (k, n_features)\n",
    "        The cluster centers. This array is MODIFIED IN PLACE\n",
    "    counts : array, shape (k,)\n",
    "         The vector in which we keep track of the numbers of elements in a\n",
    "         cluster. This array is MODIFIED IN PLACE\n",
    "    distances : array, dtype float, shape (n_samples), optional\n",
    "        If not None, should be a pre-allocated array that will be used to store\n",
    "        the distances of each sample to its closest center.\n",
    "        May not be None when random_reassign is True.\n",
    "    random_state : int, RandomState instance or None (default)\n",
    "        Determines random number generation for centroid initialization and to\n",
    "        pick new clusters amongst observations with uniform probability. Use\n",
    "        an int to make the randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    random_reassign : boolean, optional\n",
    "        If True, centers with very low counts are randomly reassigned\n",
    "        to observations.\n",
    "    reassignment_ratio : float, optional\n",
    "        Control the fraction of the maximum number of counts for a\n",
    "        center to be reassigned. A higher value means that low count\n",
    "        centers are more likely to be reassigned, which means that the\n",
    "        model will take longer to converge, but should converge in a\n",
    "        better clustering.\n",
    "    verbose : bool, optional, default False\n",
    "        Controls the verbosity.\n",
    "    compute_squared_diff : bool\n",
    "        If set to False, the squared diff computation is skipped.\n",
    "    old_center_buffer : int\n",
    "        Copy of old centers for monitoring convergence.\n",
    "    Returns\n",
    "    -------\n",
    "    inertia : float\n",
    "        Sum of squared distances of samples to their closest cluster center.\n",
    "    squared_diff : numpy array, shape (n_clusters,)\n",
    "        Squared distances between previous and updated cluster centers.\n",
    "    \"\"\"\n",
    "    # Perform label assignment to nearest centers\n",
    "    nearest_center, inertia = _labels_inertia(X, sample_weight,\n",
    "                                              x_squared_norms, centers,\n",
    "                                              distances=distances)\n",
    "\n",
    "    if random_reassign and reassignment_ratio > 0:\n",
    "        random_state = check_random_state(random_state)\n",
    "        # Reassign clusters that have very low weight\n",
    "        to_reassign = weight_sums < reassignment_ratio * weight_sums.max()\n",
    "        # pick at most .5 * batch_size samples as new centers\n",
    "        if to_reassign.sum() > .5 * X.shape[0]:\n",
    "            indices_dont_reassign = \\\n",
    "                    np.argsort(weight_sums)[int(.5 * X.shape[0]):]\n",
    "            to_reassign[indices_dont_reassign] = False\n",
    "        n_reassigns = to_reassign.sum()\n",
    "        if n_reassigns:\n",
    "            # Pick new clusters amongst observations with uniform probability\n",
    "            new_centers = random_state.choice(X.shape[0], replace=False,\n",
    "                                              size=n_reassigns)\n",
    "            if verbose:\n",
    "                print(\"[MiniBatchKMeans] Reassigning %i cluster centers.\"\n",
    "                      % n_reassigns)\n",
    "\n",
    "            if sp.issparse(X) and not sp.issparse(centers):\n",
    "                assign_rows_csr(\n",
    "                        X, new_centers.astype(np.intp, copy=False),\n",
    "                        np.where(to_reassign)[0].astype(np.intp, copy=False),\n",
    "                        centers)\n",
    "            else:\n",
    "                centers[to_reassign] = X[new_centers]\n",
    "        # reset counts of reassigned centers, but don't reset them too small\n",
    "        # to avoid instant reassignment. This is a pretty dirty hack as it\n",
    "        # also modifies the learning rates.\n",
    "        weight_sums[to_reassign] = np.min(weight_sums[~to_reassign])\n",
    "\n",
    "    # implementation for the sparse CSR representation completely written in\n",
    "    # cython\n",
    "    if sp.issparse(X):\n",
    "        return inertia, _k_means._mini_batch_update_csr(\n",
    "            X, sample_weight, x_squared_norms, centers, weight_sums,\n",
    "            nearest_center, old_center_buffer, compute_squared_diff)\n",
    "\n",
    "    # dense variant in mostly numpy (not as memory efficient though)\n",
    "    k = centers.shape[0]\n",
    "    squared_diff = 0.0\n",
    "    for center_idx in range(k):\n",
    "        # find points from minibatch that are assigned to this center\n",
    "        center_mask = nearest_center == center_idx\n",
    "        wsum = sample_weight[center_mask].sum()\n",
    "\n",
    "        if wsum > 0:\n",
    "            if compute_squared_diff:\n",
    "                old_center_buffer[:] = centers[center_idx]\n",
    "\n",
    "            # inplace remove previous count scaling\n",
    "            centers[center_idx] *= weight_sums[center_idx]\n",
    "\n",
    "            # inplace sum with new points members of this cluster\n",
    "            centers[center_idx] += \\\n",
    "                np.sum(X[center_mask] *\n",
    "                       sample_weight[center_mask, np.newaxis], axis=0)\n",
    "\n",
    "            # update the count statistics for this center\n",
    "            weight_sums[center_idx] += wsum\n",
    "\n",
    "            # inplace rescale to compute mean of all points (old and new)\n",
    "            # Note: numpy >= 1.10 does not support '/=' for the following\n",
    "            # expression for a mixture of int and float (see numpy issue #6464)\n",
    "            centers[center_idx] = centers[center_idx] / weight_sums[center_idx]\n",
    "\n",
    "            # update the squared diff if necessary\n",
    "            if compute_squared_diff:\n",
    "                diff = centers[center_idx].ravel() - old_center_buffer.ravel()\n",
    "                squared_diff += np.dot(diff, diff)\n",
    "\n",
    "    return inertia, squared_diff\n",
    "\n",
    "\n",
    "def _mini_batch_convergence(model, iteration_idx, n_iter, tol,\n",
    "                            n_samples, centers_squared_diff, batch_inertia,\n",
    "                            context, verbose=0):\n",
    "    \"\"\"Helper function to encapsulate the early stopping logic\"\"\"\n",
    "    # Normalize inertia to be able to compare values when\n",
    "    # batch_size changes\n",
    "    batch_inertia /= model.batch_size\n",
    "    centers_squared_diff /= model.batch_size\n",
    "\n",
    "    # Compute an Exponentially Weighted Average of the squared\n",
    "    # diff to monitor the convergence while discarding\n",
    "    # minibatch-local stochastic variability:\n",
    "    # https://en.wikipedia.org/wiki/Moving_average\n",
    "    ewa_diff = context.get('ewa_diff')\n",
    "    ewa_inertia = context.get('ewa_inertia')\n",
    "    if ewa_diff is None:\n",
    "        ewa_diff = centers_squared_diff\n",
    "        ewa_inertia = batch_inertia\n",
    "    else:\n",
    "        alpha = float(model.batch_size) * 2.0 / (n_samples + 1)\n",
    "        alpha = 1.0 if alpha > 1.0 else alpha\n",
    "        ewa_diff = ewa_diff * (1 - alpha) + centers_squared_diff * alpha\n",
    "        ewa_inertia = ewa_inertia * (1 - alpha) + batch_inertia * alpha\n",
    "\n",
    "    # Log progress to be able to monitor convergence\n",
    "    if verbose:\n",
    "        progress_msg = (\n",
    "            'Minibatch iteration %d/%d:'\n",
    "            ' mean batch inertia: %f, ewa inertia: %f ' % (\n",
    "                iteration_idx + 1, n_iter, batch_inertia,\n",
    "                ewa_inertia))\n",
    "        print(progress_msg)\n",
    "\n",
    "    # Early stopping based on absolute tolerance on squared change of\n",
    "    # centers position (using EWA smoothing)\n",
    "    if tol > 0.0 and ewa_diff <= tol:\n",
    "        if verbose:\n",
    "            print('Converged (small centers change) at iteration %d/%d'\n",
    "                  % (iteration_idx + 1, n_iter))\n",
    "        return True\n",
    "\n",
    "    # Early stopping heuristic due to lack of improvement on smoothed inertia\n",
    "    ewa_inertia_min = context.get('ewa_inertia_min')\n",
    "    no_improvement = context.get('no_improvement', 0)\n",
    "    if ewa_inertia_min is None or ewa_inertia < ewa_inertia_min:\n",
    "        no_improvement = 0\n",
    "        ewa_inertia_min = ewa_inertia\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "\n",
    "    if (model.max_no_improvement is not None\n",
    "            and no_improvement >= model.max_no_improvement):\n",
    "        if verbose:\n",
    "            print('Converged (lack of improvement in inertia)'\n",
    "                  ' at iteration %d/%d'\n",
    "                  % (iteration_idx + 1, n_iter))\n",
    "        return True\n",
    "\n",
    "    # update the convergence context to maintain state across successive calls:\n",
    "    context['ewa_diff'] = ewa_diff\n",
    "    context['ewa_inertia'] = ewa_inertia\n",
    "    context['ewa_inertia_min'] = ewa_inertia_min\n",
    "    context['no_improvement'] = no_improvement\n",
    "    return False\n",
    "\n",
    "\n",
    "class MiniBatchKMeans(KMeans):\n",
    "    \"\"\"\n",
    "    Mini-Batch K-Means clustering.\n",
    "    Read more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters : int, default=8\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    init : {'k-means++', 'random'} or ndarray of shape \\\n",
    "            (n_clusters, n_features), default='k-means++'\n",
    "        Method for initialization\n",
    "        'k-means++' : selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence. See section\n",
    "        Notes in k_init for more details.\n",
    "        'random': choose k observations (rows) at random from data for\n",
    "        the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
    "        and gives the initial centers.\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations over the complete dataset before\n",
    "        stopping independently of any early stopping criterion heuristics.\n",
    "    batch_size : int, default=100\n",
    "        Size of the mini batches.\n",
    "    verbose : int, default=0\n",
    "        Verbosity mode.\n",
    "    compute_labels : bool, default=True\n",
    "        Compute label assignment and inertia for the complete dataset\n",
    "        once the minibatch optimization has converged in fit.\n",
    "    random_state : int, RandomState instance, default=None\n",
    "        Determines random number generation for centroid initialization and\n",
    "        random reassignment. Use an int to make the randomness deterministic.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    tol : float, default=0.0\n",
    "        Control early stopping based on the relative center changes as\n",
    "        measured by a smoothed, variance-normalized of the mean center\n",
    "        squared position changes. This early stopping heuristics is\n",
    "        closer to the one used for the batch variant of the algorithms\n",
    "        but induces a slight computational and memory overhead over the\n",
    "        inertia heuristic.\n",
    "        To disable convergence detection based on normalized center\n",
    "        change, set tol to 0.0 (default).\n",
    "    max_no_improvement : int, default=10\n",
    "        Control early stopping based on the consecutive number of mini\n",
    "        batches that does not yield an improvement on the smoothed inertia.\n",
    "        To disable convergence detection based on inertia, set\n",
    "        max_no_improvement to None.\n",
    "    init_size : int, default=None\n",
    "        Number of samples to randomly sample for speeding up the\n",
    "        initialization (sometimes at the expense of accuracy): the\n",
    "        only algorithm is initialized by running a batch KMeans on a\n",
    "        random subset of the data. This needs to be larger than n_clusters.\n",
    "        If `None`, `init_size= 3 * batch_size`.\n",
    "    n_init : int, default=3\n",
    "        Number of random initializations that are tried.\n",
    "        In contrast to KMeans, the algorithm is only run once, using the\n",
    "        best of the ``n_init`` initializations as measured by inertia.\n",
    "    reassignment_ratio : float, default=0.01\n",
    "        Control the fraction of the maximum number of counts for a\n",
    "        center to be reassigned. A higher value means that low count\n",
    "        centers are more easily reassigned, which means that the\n",
    "        model will take longer to converge, but should converge in a\n",
    "        better clustering.\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
    "        Coordinates of cluster centers\n",
    "    labels_ : int\n",
    "        Labels of each point (if compute_labels is set to True).\n",
    "    inertia_ : float\n",
    "        The value of the inertia criterion associated with the chosen\n",
    "        partition (if compute_labels is set to True). The inertia is\n",
    "        defined as the sum of square distances of samples to their nearest\n",
    "        neighbor.\n",
    "    See Also\n",
    "    --------\n",
    "    KMeans\n",
    "        The classic implementation of the clustering method based on the\n",
    "        Lloyd's algorithm. It consumes the whole set of input data at each\n",
    "        iteration.\n",
    "    Notes\n",
    "    -----\n",
    "    See https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.cluster import MiniBatchKMeans\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "    ...               [4, 2], [4, 0], [4, 4],\n",
    "    ...               [4, 5], [0, 1], [2, 2],\n",
    "    ...               [3, 2], [5, 5], [1, -1]])\n",
    "    >>> # manually fit on batches\n",
    "    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "    ...                          random_state=0,\n",
    "    ...                          batch_size=6)\n",
    "    >>> kmeans = kmeans.partial_fit(X[0:6,:])\n",
    "    >>> kmeans = kmeans.partial_fit(X[6:12,:])\n",
    "    >>> kmeans.cluster_centers_\n",
    "    array([[2. , 1. ],\n",
    "           [3.5, 4.5]])\n",
    "    >>> kmeans.predict([[0, 0], [4, 4]])\n",
    "    array([0, 1], dtype=int32)\n",
    "    >>> # fit on the whole data\n",
    "    >>> kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "    ...                          random_state=0,\n",
    "    ...                          batch_size=6,\n",
    "    ...                          max_iter=10).fit(X)\n",
    "    >>> kmeans.cluster_centers_\n",
    "    array([[3.95918367, 2.40816327],\n",
    "           [1.12195122, 1.3902439 ]])\n",
    "    >>> kmeans.predict([[0, 0], [4, 4]])\n",
    "    array([1, 0], dtype=int32)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=8, init='k-means++', max_iter=100,\n",
    "                 batch_size=100, verbose=0, compute_labels=True,\n",
    "                 random_state=None, tol=0.0, max_no_improvement=10,\n",
    "                 init_size=None, n_init=3, reassignment_ratio=0.01):\n",
    "\n",
    "        super().__init__(\n",
    "            n_clusters=n_clusters, init=init, max_iter=max_iter,\n",
    "            verbose=verbose, random_state=random_state, tol=tol, n_init=n_init)\n",
    "\n",
    "        self.max_no_improvement = max_no_improvement\n",
    "        self.batch_size = batch_size\n",
    "        self.compute_labels = compute_labels\n",
    "        self.init_size = init_size\n",
    "        self.reassignment_ratio = reassignment_ratio\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Compute the centroids on X by chunking it into mini-batches.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "            Training instances to cluster. It must be noted that the data\n",
    "            will be converted to C ordering, which will cause a memory copy\n",
    "            if the given data is not C-contiguous.\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        X = check_array(X, accept_sparse=\"csr\", order='C',\n",
    "                        dtype=[np.float64, np.float32])\n",
    "        n_samples, n_features = X.shape\n",
    "        if n_samples < self.n_clusters:\n",
    "            raise ValueError(\"n_samples=%d should be >= n_clusters=%d\"\n",
    "                             % (n_samples, self.n_clusters))\n",
    "\n",
    "        sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "\n",
    "        n_init = self.n_init\n",
    "        if hasattr(self.init, '__array__'):\n",
    "            self.init = np.ascontiguousarray(self.init, dtype=X.dtype)\n",
    "            if n_init != 1:\n",
    "                warnings.warn(\n",
    "                    'Explicit initial center position passed: '\n",
    "                    'performing only one init in MiniBatchKMeans instead of '\n",
    "                    'n_init=%d'\n",
    "                    % self.n_init, RuntimeWarning, stacklevel=2)\n",
    "                n_init = 1\n",
    "\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "        if self.tol > 0.0:\n",
    "            tol = _tolerance(X, self.tol)\n",
    "\n",
    "            # using tol-based early stopping needs the allocation of a\n",
    "            # dedicated before which can be expensive for high dim data:\n",
    "            # hence we allocate it outside of the main loop\n",
    "            old_center_buffer = np.zeros(n_features, dtype=X.dtype)\n",
    "        else:\n",
    "            tol = 0.0\n",
    "            # no need for the center buffer if tol-based early stopping is\n",
    "            # disabled\n",
    "            old_center_buffer = np.zeros(0, dtype=X.dtype)\n",
    "\n",
    "        distances = np.zeros(self.batch_size, dtype=X.dtype)\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        n_iter = int(self.max_iter * n_batches)\n",
    "\n",
    "        init_size = self.init_size\n",
    "        if init_size is None:\n",
    "            init_size = 3 * self.batch_size\n",
    "        if init_size > n_samples:\n",
    "            init_size = n_samples\n",
    "        self.init_size_ = init_size\n",
    "\n",
    "        validation_indices = random_state.randint(0, n_samples, init_size)\n",
    "        X_valid = X[validation_indices]\n",
    "        sample_weight_valid = sample_weight[validation_indices]\n",
    "        x_squared_norms_valid = x_squared_norms[validation_indices]\n",
    "\n",
    "        # perform several inits with random sub-sets\n",
    "        best_inertia = None\n",
    "        for init_idx in range(n_init):\n",
    "            if self.verbose:\n",
    "                print(\"Init %d/%d with method: %s\"\n",
    "                      % (init_idx + 1, n_init, self.init))\n",
    "            weight_sums = np.zeros(self.n_clusters, dtype=sample_weight.dtype)\n",
    "\n",
    "            # TODO: once the `k_means` function works with sparse input we\n",
    "            # should refactor the following init to use it instead.\n",
    "\n",
    "            # Initialize the centers using only a fraction of the data as we\n",
    "            # expect n_samples to be very large when using MiniBatchKMeans\n",
    "            cluster_centers = _init_centroids(\n",
    "                X, self.n_clusters, self.init,\n",
    "                random_state=random_state,\n",
    "                x_squared_norms=x_squared_norms,\n",
    "                init_size=init_size)\n",
    "\n",
    "            # Compute the label assignment on the init dataset\n",
    "            _mini_batch_step(\n",
    "                X_valid, sample_weight_valid,\n",
    "                x_squared_norms[validation_indices], cluster_centers,\n",
    "                weight_sums, old_center_buffer, False, distances=None,\n",
    "                verbose=self.verbose)\n",
    "\n",
    "            # Keep only the best cluster centers across independent inits on\n",
    "            # the common validation set\n",
    "            _, inertia = _labels_inertia(X_valid, sample_weight_valid,\n",
    "                                         x_squared_norms_valid,\n",
    "                                         cluster_centers)\n",
    "            if self.verbose:\n",
    "                print(\"Inertia for init %d/%d: %f\"\n",
    "                      % (init_idx + 1, n_init, inertia))\n",
    "            if best_inertia is None or inertia < best_inertia:\n",
    "                self.cluster_centers_ = cluster_centers\n",
    "                self.counts_ = weight_sums\n",
    "                best_inertia = inertia\n",
    "\n",
    "        # Empty context to be used inplace by the convergence check routine\n",
    "        convergence_context = {}\n",
    "\n",
    "        # Perform the iterative optimization until the final convergence\n",
    "        # criterion\n",
    "        for iteration_idx in range(n_iter):\n",
    "            # Sample a minibatch from the full dataset\n",
    "            minibatch_indices = random_state.randint(\n",
    "                0, n_samples, self.batch_size)\n",
    "\n",
    "            # Perform the actual update step on the minibatch data\n",
    "            batch_inertia, centers_squared_diff = _mini_batch_step(\n",
    "                X[minibatch_indices], sample_weight[minibatch_indices],\n",
    "                x_squared_norms[minibatch_indices],\n",
    "                self.cluster_centers_, self.counts_,\n",
    "                old_center_buffer, tol > 0.0, distances=distances,\n",
    "                # Here we randomly choose whether to perform\n",
    "                # random reassignment: the choice is done as a function\n",
    "                # of the iteration index, and the minimum number of\n",
    "                # counts, in order to force this reassignment to happen\n",
    "                # every once in a while\n",
    "                random_reassign=((iteration_idx + 1)\n",
    "                                 % (10 + int(self.counts_.min())) == 0),\n",
    "                random_state=random_state,\n",
    "                reassignment_ratio=self.reassignment_ratio,\n",
    "                verbose=self.verbose)\n",
    "\n",
    "            # Monitor convergence and do early stopping if necessary\n",
    "            if _mini_batch_convergence(\n",
    "                    self, iteration_idx, n_iter, tol, n_samples,\n",
    "                    centers_squared_diff, batch_inertia, convergence_context,\n",
    "                    verbose=self.verbose):\n",
    "                break\n",
    "\n",
    "        self.n_iter_ = iteration_idx + 1\n",
    "\n",
    "        if self.compute_labels:\n",
    "            self.labels_, self.inertia_ = \\\n",
    "                    self._labels_inertia_minibatch(X, sample_weight)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _labels_inertia_minibatch(self, X, sample_weight):\n",
    "        \"\"\"Compute labels and inertia using mini batches.\n",
    "        This is slightly slower than doing everything at once but preventes\n",
    "        memory errors / segfaults.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data.\n",
    "        sample_weight : array-like, shape (n_samples,)\n",
    "            The weights for each observation in X.\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape (n_samples,)\n",
    "            Cluster labels for each point.\n",
    "        inertia : float\n",
    "            Sum of squared distances of points to nearest cluster.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print('Computing label assignment and total inertia')\n",
    "        sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "        slices = gen_batches(X.shape[0], self.batch_size)\n",
    "        results = [_labels_inertia(X[s], sample_weight[s], x_squared_norms[s],\n",
    "                                   self.cluster_centers_) for s in slices]\n",
    "        labels, inertia = zip(*results)\n",
    "        return np.hstack(labels), np.sum(inertia)\n",
    "\n",
    "    def partial_fit(self, X, y=None, sample_weight=None):\n",
    "        \"\"\"Update k means estimate on a single mini-batch X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Coordinates of the data points to cluster. It must be noted that\n",
    "            X will be copied if it is not C-contiguous.\n",
    "        y : Ignored\n",
    "            Not used, present here for API consistency by convention.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        X = check_array(X, accept_sparse=\"csr\", order=\"C\",\n",
    "                        dtype=[np.float64, np.float32])\n",
    "        n_samples, n_features = X.shape\n",
    "        if hasattr(self.init, '__array__'):\n",
    "            self.init = np.ascontiguousarray(self.init, dtype=X.dtype)\n",
    "\n",
    "        if n_samples == 0:\n",
    "            return self\n",
    "\n",
    "        sample_weight = _check_normalize_sample_weight(sample_weight, X)\n",
    "\n",
    "        x_squared_norms = row_norms(X, squared=True)\n",
    "        self.random_state_ = getattr(self, \"random_state_\",\n",
    "                                     check_random_state(self.random_state))\n",
    "        if (not hasattr(self, 'counts_')\n",
    "                or not hasattr(self, 'cluster_centers_')):\n",
    "            # this is the first call partial_fit on this object:\n",
    "            # initialize the cluster centers\n",
    "            self.cluster_centers_ = _init_centroids(\n",
    "                X, self.n_clusters, self.init,\n",
    "                random_state=self.random_state_,\n",
    "                x_squared_norms=x_squared_norms, init_size=self.init_size)\n",
    "\n",
    "            self.counts_ = np.zeros(self.n_clusters,\n",
    "                                    dtype=sample_weight.dtype)\n",
    "            random_reassign = False\n",
    "            distances = None\n",
    "        else:\n",
    "            # The lower the minimum count is, the more we do random\n",
    "            # reassignment, however, we don't want to do random\n",
    "            # reassignment too often, to allow for building up counts\n",
    "            random_reassign = self.random_state_.randint(\n",
    "                10 * (1 + self.counts_.min())) == 0\n",
    "            distances = np.zeros(X.shape[0], dtype=X.dtype)\n",
    "\n",
    "        _mini_batch_step(X, sample_weight, x_squared_norms,\n",
    "                         self.cluster_centers_, self.counts_,\n",
    "                         np.zeros(0, dtype=X.dtype), 0,\n",
    "                         random_reassign=random_reassign, distances=distances,\n",
    "                         random_state=self.random_state_,\n",
    "                         reassignment_ratio=self.reassignment_ratio,\n",
    "                         verbose=self.verbose)\n",
    "\n",
    "        if self.compute_labels:\n",
    "            self.labels_, self.inertia_ = _labels_inertia(\n",
    "                X, sample_weight, x_squared_norms, self.cluster_centers_)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, sample_weight=None):\n",
    "        \"\"\"Predict the closest cluster each sample in X belongs to.\n",
    "        In the vector quantization literature, `cluster_centers_` is called\n",
    "        the code book and each value returned by `predict` is the index of\n",
    "        the closest code in the code book.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            New data to predict.\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            The weights for each observation in X. If None, all observations\n",
    "            are assigned equal weight (default: None).\n",
    "        Returns\n",
    "        -------\n",
    "        labels : array, shape [n_samples,]\n",
    "            Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._check_test_data(X)\n",
    "        return self._labels_inertia_minibatch(X, sample_weight)[0]\n",
    " \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmean on iris DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.6 3.4]\n",
      " [4.6 3.1]\n",
      " [5.7 2.5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2098fc82e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2098fad2320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target\n",
    "\n",
    "xmin1 = X[0].min()\n",
    "xmin2 = X[1].min()\n",
    "#print(xmin1,xmin2)\n",
    "\n",
    "xmax1 = X[0].max()\n",
    "xmax2 = X[1].max()\n",
    "#print(xmax1,xmax2)\n",
    "\n",
    "#xmean1 = X[0].mean()\n",
    "#xmean2 = X[1].mean()\n",
    "#print('mean',xmean1,xmean2)\n",
    "\n",
    "#xstd1 = X[0].std()\n",
    "#xstd2 = X[1].std()\n",
    "#print('std',xstd1,xstd2)\n",
    "\n",
    "#x_1 = np.random.uniform(low=xmin1, high=xmax1,size=3)\n",
    "#x_2 = np.random.uniform(low=xmin2, high=xmax2,size=3)\n",
    "df = pd.DataFrame(X)\n",
    "centers = df.sample(3,random_state=2)\n",
    "# print('sample')\n",
    "# print(centers)\n",
    "# print('************************')\n",
    "# print(centers.iloc[0][0],centers.iloc[0][1])\n",
    "\n",
    "\n",
    "#np.random.randint(low=1, high=100, size=4)\n",
    "#np.random.randint(low=1, high=100, size=4)\n",
    "\n",
    "k = 3\n",
    "\n",
    "#centroids = np.array([[xmin1,xmin2], [xmean1,xmean2] ,[xmax1,xmax2]]) \n",
    "centroids = np.array([[centers.iloc[0][0],centers.iloc[0][1]], [centers.iloc[1][0],centers.iloc[1][1]] ,[centers.iloc[2][0],centers.iloc[2][1]]]) \n",
    "print(centroids)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],marker='+',c=y, s=50)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker='*', c='g', s=150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter 0 delta_inertia 1\n",
      "current_inertia 57.557089965172686 prev_inertia 1\n",
      "counter 1 delta_inertia 56.557089965172686\n",
      "current_inertia 51.40169599217988 prev_inertia 57.557089965172686\n",
      "counter 2 delta_inertia 6.155393972992805\n",
      "current_inertia 49.12424760601917 prev_inertia 51.40169599217988\n",
      "counter 3 delta_inertia 2.2774483861607138\n",
      "current_inertia 47.39728592545666 prev_inertia 49.12424760601917\n",
      "counter 4 delta_inertia 1.7269616805625105\n",
      "current_inertia 45.745424106274555 prev_inertia 47.39728592545666\n",
      "counter 5 delta_inertia 1.651861819182102\n",
      "current_inertia 44.350734364925856 prev_inertia 45.745424106274555\n",
      "counter 6 delta_inertia 1.3946897413486994\n",
      "current_inertia 43.60639348370927 prev_inertia 44.350734364925856\n",
      "counter 7 delta_inertia 0.744340881216587\n",
      "current_inertia 41.33018939393939 prev_inertia 43.60639348370927\n",
      "counter 8 delta_inertia 2.2762040897698768\n",
      "current_inertia 38.95803126213265 prev_inertia 41.33018939393939\n",
      "counter 9 delta_inertia 2.372158131806742\n",
      "current_inertia 38.13828328939231 prev_inertia 38.95803126213265\n",
      "counter 10 delta_inertia 0.8197479727403376\n",
      "current_inertia 37.61932103610675 prev_inertia 38.13828328939231\n",
      "counter 11 delta_inertia 0.5189622532855651\n",
      "current_inertia 37.303602825745685 prev_inertia 37.61932103610675\n",
      "counter 12 delta_inertia 0.3157182103610623\n",
      "current_inertia 37.0876858974359 prev_inertia 37.303602825745685\n",
      "counter 13 delta_inertia 0.21591692830978815\n",
      "current_inertia 37.05070212765958 prev_inertia 37.0876858974359\n",
      "counter 14 delta_inertia 0.03698376977632023\n",
      "current_inertia 37.05070212765958 prev_inertia 37.05070212765958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.006     , 3.428     ],\n",
       "       [5.77358491, 2.69245283],\n",
       "       [6.81276596, 3.07446809]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final delta_inertia 0.0 counter 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2098fd55cc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2098fcd4080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "centroids_old = np.zeros(centroids.shape) # to store old centers\n",
    "#display('centroids_old',centroids_old)\n",
    "centroids_new = centroids.copy() # Store new centers\n",
    "#display('centroids_new',centroids_new)\n",
    "\n",
    "n = X.shape[0]\n",
    "clusters = np.zeros(n)\n",
    "distances = np.zeros((n,k))\n",
    "inertia = np.zeros((k,1))\n",
    "#display('clusters',clusters)\n",
    "#display('distances',distances)\n",
    "\n",
    "#print(centroids_new - centroids_old)\n",
    "#print(((centroids_new - centroids_old)**2))\n",
    "#print(((centroids_new - centroids_old)**2).sum(axis=1))\n",
    "\n",
    "tolerance = 0.0001\n",
    "prev_inertia, current_inertia = 0,1 \n",
    "\n",
    "delta_inertia = np.abs(current_inertia - prev_inertia) #np.sqrt(((centroids_new - centroids_old)**2).sum(axis=1)) \n",
    "#display('error',error)\n",
    "counter = 0\n",
    "while delta_inertia > tolerance:\n",
    "    print('counter', counter,'delta_inertia',delta_inertia)\n",
    "    #display('X',X)\n",
    "    for i in range(k):\n",
    "    #         print(data - centroids[i])\n",
    "    #         print(((data - centroids[i])**2))\n",
    "    #         print(((data - centroids[i])**2).sum(axis=1))\n",
    "    #         print(np.sqrt(((data - centroids[i])**2).sum(axis=1)))\n",
    "        distances[:,i] = np.sqrt(((X - centroids_new[i])**2).sum(axis=1))\n",
    "        #print('distances k=',i,distances[:,i])\n",
    "\n",
    "    clusters = np.argmin(distances, axis = 1)\n",
    "    #display('clusters argmin=',clusters)\n",
    "\n",
    "    centroids_old = centroids_new.copy()\n",
    "\n",
    "    for i in range(k):\n",
    "#         print(X[clusters == 1])\n",
    "#         print(np.mean(X[clusters == i], axis=0))\n",
    "        centroids_new[i] = np.mean(X[clusters == i], axis=0)\n",
    "   \n",
    "        \n",
    "    for i in range(k):\n",
    "        \n",
    "        idx = np.where(clusters==i)\n",
    "        #print('idx',idx)\n",
    "        inertia[i] = np.sum(((X[idx] - centroids_new[i])**2).sum(axis=1))\n",
    "        #print('inertia',inertia)\n",
    "\n",
    "    prev_inertia = current_inertia\n",
    "    current_inertia = inertia.sum()\n",
    "    delta_inertia = np.abs(current_inertia - prev_inertia)\n",
    "    counter += 1\n",
    "    print('current_inertia',current_inertia,'prev_inertia',prev_inertia)\n",
    "\n",
    "display(centroids_new)\n",
    "print('final delta_inertia',delta_inertia,'counter',counter)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1],marker='+',c=y, s=50)\n",
    "plt.scatter(centroids_new[:,0], centroids_new[:,1], marker='*', c='g', s=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans sample page 397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "centroids = np.array([[0,0],[16,0],[16,6]],dtype=float)\n",
    "data = np.array([[0,0],[8,0],[8,6],[16,0],[0,6],[16,6],[5,5]])\n",
    "\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1],marker='+', s=50)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker='*', c='g', s=150)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run k = 1\n",
    "\n",
    "centroids_old = np.zeros(centroids.shape) # to store old centers\n",
    "display('centroids_old',centroids_old)\n",
    "centroids_new = centroids.copy() # Store new centers\n",
    "display('centroids_new',centroids_new)\n",
    "\n",
    "n = data.shape[0]\n",
    "clusters = np.zeros(n)\n",
    "distances = np.zeros((n,k))\n",
    "inertia = np.zeros((k,1))\n",
    "#display('clusters',clusters)\n",
    "#display('distances',distances)\n",
    "\n",
    "#print(centroids_new - centroids_old)\n",
    "#print(((centroids_new - centroids_old)**2))\n",
    "#print(((centroids_new - centroids_old)**2).sum(axis=1))\n",
    "\n",
    "tolerance = 0.0001\n",
    "prev_inertia, current_inertia = 0,1 \n",
    "\n",
    "delta_inertia = current_inertia - prev_inertia #np.sqrt(((centroids_new - centroids_old)**2).sum(axis=1)) \n",
    "#display('error',error)\n",
    "\n",
    "while delta_inertia > tolerance:\n",
    "\n",
    "    display('data',data)\n",
    "    for i in range(k):\n",
    "    #         print(data - centroids[i])\n",
    "    #         print(((data - centroids[i])**2))\n",
    "    #         print(((data - centroids[i])**2).sum(axis=1))\n",
    "    #         print(np.sqrt(((data - centroids[i])**2).sum(axis=1)))\n",
    "        distances[:,i] = np.sqrt(((data - centroids[i])**2).sum(axis=1))\n",
    "        print('distances k=',i,distances[:,i])\n",
    "\n",
    "    clusters = np.argmin(distances, axis = 1)\n",
    "    display('clusters argmin=',clusters)\n",
    "\n",
    "    centroids_old = centroids_new.copy()\n",
    "\n",
    "    for i in range(k):\n",
    "        print(np.mean(data[clusters == i], axis=0))\n",
    "        centroids_new[i] = np.mean(data[clusters == i], axis=0)\n",
    "\n",
    "    display(centroids_new)\n",
    "    error -= 1 \n",
    "    \n",
    "    for i in range(k):\n",
    "        \n",
    "        idx = np.where(clusters==i)\n",
    "        print('idx',idx)\n",
    "        inertia[i] = np.sum(((data[idx] - centroids_new[i])**2).sum(axis=1))\n",
    "        print('inertia',inertia)\n",
    "\n",
    "    prev_inertia = current_inertia\n",
    "    current_inertia = inertia.sum()\n",
    "    delta_inertia = current_inertia - prev_inertia\n",
    "    print('current_inertia',current_inertia)\n",
    "\n",
    "print('final delta_inertia',delta_inertia)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1],marker='+', s=50)\n",
    "plt.scatter(centroids_new[:,0], centroids_new[:,1], marker='*', c='g', s=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
