{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import  StratifiedShuffleSplit,GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(70000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(35, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(35, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([7., 7., 0., 1., 1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_path = 'mnist-original.mat'\n",
    "\n",
    "mnist = loadmat(mnist_path)\n",
    "X = mnist['data'][:5].T\n",
    "y = mnist['label'][:5][0]\n",
    "display(X.shape , y.shape)\n",
    "\n",
    "# Scale all X values\n",
    "scaler = StandardScaler()\n",
    "X_scaled  = scaler.fit_transform(X)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.0005,train_size=0.0005, random_state=0)\n",
    "train_index, test_index = next(sss.split(X=X_scaled, y=y))   \n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "display(X_train.shape,X_test.shape)\n",
    "\n",
    "display(y_train[:5])\n",
    "df_y_train = pd.get_dummies(y_train)\n",
    "y_train = df_y_train.values\n",
    "display(y_train[:5])\n",
    "\n",
    "df_y_test = pd.get_dummies(y_test)\n",
    "y_test = df_y_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPerceptron(BaseEstimator):\n",
    "    \n",
    "    #layers - includes input, #hidden, output\n",
    "    def __init__(self,layers,eta=0,random_state=50):\n",
    "        self.random_state = random_state\n",
    "        self.eta = eta\n",
    "        self.n_layers = len(layers)\n",
    "        self.layers = layers\n",
    "       \n",
    "        self.init_weights()\n",
    "        self.initialize_theta_weights()\n",
    "        print('n_layers',self.n_layers)\n",
    "        \n",
    "    def initialize_theta_weights(self):\n",
    "        '''\n",
    "        Initialize theta_weights, initialization method depends\n",
    "        on the Activation Function and the Number of Units in the current layer\n",
    "        and the next layer.\n",
    "        The weights for each layer as of the size [next_layer, current_layer + 1]\n",
    "        '''\n",
    "        print('initialize_theta_weights start')\n",
    "        self.theta_weights = []\n",
    "        size_next_layers = self.layers.copy()\n",
    "        size_next_layers.pop(0)\n",
    "        for size_layer, size_next_layer in zip(self.layers, size_next_layers):\n",
    "            print(size_layer,size_next_layer)\n",
    "           \n",
    "            epsilon = 4.0 * np.sqrt(6) / np.sqrt(size_layer + size_next_layer)\n",
    "            print('epsilon',epsilon)\n",
    "            # Weigts from a uniform distribution [-epsilon, epsion]\n",
    "            \n",
    "            theta_tmp = epsilon * ( (np.random.rand(size_next_layer, size_layer + 1) * 2.0 ) - 1)\n",
    "            print('theta_tmp',theta_tmp)\n",
    "            \n",
    "        self.theta_weights.append(theta_tmp)\n",
    "        print(self.theta_weights)\n",
    "        print('initialize_theta_weights end')\n",
    "        return self.theta_weights\n",
    "    \n",
    "    def init_weights(self):\n",
    "        #rng = np.random.RandomState(self.random_state)       \n",
    "        #self.weights = np.round(rng.normal(loc=0.0,scale=0.1,size=X.shape[0] + 1),4)\n",
    "        self.weights = np.array([0.5,0.2,0.3])\n",
    "        \n",
    "        print('X shape',X.shape[0])\n",
    "        print('init_weights',self.weights.shape)\n",
    "        \n",
    "        return self.weights\n",
    "    \n",
    "    def train(self,X,y,n_iter=50):\n",
    "    \n",
    "        n_samples = y.shape[0]\n",
    "        print('n_iter',n_iter)\n",
    "        \n",
    "        for it in range(n_iter):\n",
    "            print('train it',it)\n",
    "            self.gradients = self.back_propagation(X, y)\n",
    "            \"\"\"\n",
    "            self.gradients_vector = self.unroll_weights(self.gradients)\n",
    "            self.theta_vector = self.unroll_weights(self.theta_weights)\n",
    "            self.theta_vector = self.theta_vector - self.gradients_vector\n",
    "            self.theta_weights = self.roll_weights(self.theta_vector)\n",
    "            \"\"\"\n",
    "            \n",
    "    def back_propagation(self, X,y):\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        print('x shape',n_samples)\n",
    "    \n",
    "        A, Z = self.foward_propagation(X)\n",
    "\n",
    "        \"\"\"\n",
    "        # Backpropagation\n",
    "        deltas = [None] * self.n_layers\n",
    "        deltas[-1] = A[-1] - Y\n",
    "        # For the second last layer to the second one\n",
    "        for ix_layer in np.arange(self.n_layers - 1 - 1 , 0 , -1):\n",
    "            theta_tmp = self.theta_weights[ix_layer]\n",
    "            if self.bias_flag:\n",
    "                # Removing weights for bias\n",
    "                theta_tmp = np.delete(theta_tmp, np.s_[0], 1)\n",
    "            deltas[ix_layer] = (np.matmul(theta_tmp.transpose(), deltas[ix_layer + 1].transpose() ) ).transpose() * self.error_part1(Z[ix_layer])\n",
    "\n",
    "        # Compute gradients\n",
    "        gradients = [None] * (self.n_layers - 1)\n",
    "        for ix_layer in range(self.n_layers - 1):\n",
    "            grads_tmp = np.matmul(deltas[ix_layer + 1].transpose() , A[ix_layer])\n",
    "            grads_tmp = grads_tmp / n_examples\n",
    "            if self.bias_flag:\n",
    "                # Regularize weights, except for bias weigths\n",
    "                grads_tmp[:, 1:] = grads_tmp[:, 1:] + (self.lambda_r / n_examples) * self.theta_weights[ix_layer][:,1:]\n",
    "            else:\n",
    "                # Regularize ALL weights\n",
    "                grads_tmp = grads_tmp + (self.lambda_r / n_examples) * self.theta_weights[ix_layer]       \n",
    "            gradients[ix_layer] = grads_tmp;\n",
    "        return gradients       \n",
    "        \"\"\"\n",
    "        \n",
    "    def foward_propagation(self, X):\n",
    "        \n",
    "        print('foward_propagation n_layers',self.n_layers)\n",
    "        \n",
    "        inputs = [None] * self.n_layers\n",
    "        nets = [None] * self.n_layers\n",
    "        \n",
    "        \n",
    "        input_data = X\n",
    "        print('foward_propagation input_data',input_data)\n",
    "        \n",
    "        \n",
    "        for idx_layer in range(self.n_layers - 1):\n",
    "            n_examples = input_data.shape[0]\n",
    "            \n",
    "            input_data = np.append(input_data, 1)\n",
    "            print('input_data',input_data)\n",
    "            \n",
    "            input_data = input_data.reshape(-1,1)\n",
    "            self.weights = self.weights.reshape(-1,1)\n",
    "            print(self.weights.transpose().shape,input_data.shape)\n",
    "           \n",
    "            \n",
    "            inputs[idx_layer] = input_data\n",
    "            \n",
    "            nets[idx_layer + 1] = self.net(input_data)\n",
    "           \n",
    "            output_data = self.sigmoid(nets[idx_layer + 1])\n",
    "            \n",
    "            print('[foward_propagation] layer',idx_layer + 1, ' net ', nets[idx_layer + 1], 'output',output_data)\n",
    "                \n",
    "            input_data = output_data\n",
    "\n",
    "        #A[self.n_layers - 1] = output_layer\n",
    "               \n",
    "        return inputs, nets\n",
    "        \n",
    "\n",
    "    \n",
    "    def predict(self,X,y):\n",
    "        pass\n",
    "    \n",
    "    def net(self,X):\n",
    "        return np.dot(self.weights.transpose(),X)\n",
    "    \n",
    "    def sigmoid(self,value):\n",
    "        sig = 1 / (1 + np.exp(-value))\n",
    "        return sig     \n",
    " \n",
    "    def error_part1(self, value):\n",
    "        err_part1 = self.sigmoid(value) * (1 - seld.sigmoid(value)) \n",
    "        return err_part1\n",
    "   \n",
    " \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame([1,0])\n",
    "print(X.values.shape)\n",
    "y = pd.DataFrame([1,0])\n",
    "print(y.values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape 2\n",
      "init_weights (3,)\n",
      "initialize_theta_weights start\n",
      "2 3\n",
      "epsilon 4.381780460041329\n",
      "theta_tmp [[ 0.09874648  4.28090146  3.01591852]\n",
      " [ 0.07189939 -2.55694854  3.52028905]\n",
      " [-2.20576646  1.83710336 -2.2290185 ]]\n",
      "3 2\n",
      "epsilon 4.381780460041329\n",
      "theta_tmp [[ 3.45218532 -3.6676145   1.16879888 -2.92929289]\n",
      " [ 2.76087586 -3.75604021 -1.31833965 -1.63146981]]\n",
      "2 1\n",
      "epsilon 5.65685424949238\n",
      "theta_tmp [[-2.92826741 -3.53841589 -1.86142306]]\n",
      "[array([[-2.92826741, -3.53841589, -1.86142306]])]\n",
      "initialize_theta_weights end\n",
      "n_layers 4\n",
      "n_iter 1\n",
      "train it 0\n",
      "x shape 2\n",
      "foward_propagation n_layers 4\n",
      "foward_propagation input_data [[1]\n",
      " [0]]\n",
      "input_data [1 0 1]\n",
      "(1, 3) (3, 1)\n",
      "[foward_propagation] layer 1  net  [[0.8]] output [[0.68997448]]\n",
      "input_data [0.68997448 1.        ]\n",
      "(1, 3) (2, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-eab5e135d5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMLPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#[784, 784, 10]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-77b8304c1031>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y, n_iter)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train it'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \"\"\"\n\u001b[0;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munroll_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-77b8304c1031>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'x shape'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfoward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n",
      "\u001b[1;32m<ipython-input-238-77b8304c1031>\u001b[0m in \u001b[0;36mfoward_propagation\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    122\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_layer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m             \u001b[0mnets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_layer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0moutput_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_layer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-238-77b8304c1031>\u001b[0m in \u001b[0;36mnet\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (1,3) and (2,1) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "mlp = MLPerceptron(layers = [2, 3,2, 1],eta=0,random_state=50) #[784, 784, 10]\n",
    "\n",
    "mlp.train(X.values, y.values,n_iter=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m45\u001b[0m\n\u001b[1;33m    def backpropagation(self, X, Y):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Parameters of MLP  #  MULTI LAYER PERCEPTRON\n",
    "Number of layers : 3 (input, hidden1, output)  כמה שכבות קלט נסתרת ופלט\n",
    "Elements in layers : [784, 100, 10] # 28*28 = 784 לוקחים את התמונה ופורסים אותה בווקטור אחד     # 100 נוריונים # השכבה האחרונה היא סופטמקס המתצמתת את המאה נוירונים ומקבלים עשרה מספרים את המאה אפשר לשחק עם זה\n",
    "Activation function : Rectified Linear function\n",
    "Regularization parameter : 0 \n",
    "Bias element added in input layers : False  # אפשר להוסיף ביס ואפשר בלי\n",
    "\n",
    "This file contains the Multi-Layer Perceptron (MLP) class which creates a\n",
    "fully-connected-feedforward-artifitial-neural-network object with methods\n",
    "for its usage\n",
    "Methods:\n",
    "    __init__()\n",
    "    train(X, y, iterations, reset)\n",
    "    predict(X)\n",
    "    initialize_theta_weights()\n",
    "    backpropagation(X, Y)\n",
    "    feedforward(X)\n",
    "    unroll_weights(rolled_data)\n",
    "    roll_weights(unrolled_data)\n",
    "    sigmoid(z)\n",
    "    relu(z)\n",
    "    sigmoid_derivative(z)\n",
    "    relu_derivative(z)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "class Mlp(): # זה קלאס MLP\n",
    "   \n",
    "     \n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Given X (feature matrix), y_hay is computed\n",
    "        Arguments:\n",
    "            X      : Feature matrix [n_examples, n_features]\n",
    "        Output:\n",
    "            y_hat  : Computed Vector Class for X    # עבור כל איטרציה נקבל וואי כובע\n",
    "        '''\n",
    "        A , Z = self.feedforward(X)\n",
    "        Y_hat = A[-1]\n",
    "        return Y_hat\n",
    "\n",
    "      \n",
    "\n",
    "  \n",
    "\n",
    "    def unroll_weights(self, rolled_data):\n",
    "        '''\n",
    "        Unroll a list of matrices to a single vector\n",
    "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
    "        '''\n",
    "        unrolled_array = np.array([])\n",
    "        for one_layer in rolled_data:\n",
    "            unrolled_array = np.concatenate((unrolled_array, one_layer.flatten(1)) )\n",
    "        return unrolled_array\n",
    "\n",
    "    def roll_weights(self, unrolled_data):\n",
    "        '''\n",
    "        Unrolls a single vector to a list of matrices\n",
    "        Each matrix represents the Weights (or Gradients) from one layer to the next\n",
    "        '''\n",
    "        size_next_layers = self.size_layers.copy()\n",
    "        size_next_layers.pop(0)\n",
    "        rolled_list = []\n",
    "        if self.bias_flag:\n",
    "            extra_item = 1\n",
    "        else:\n",
    "            extra_item = 0\n",
    "        for size_layer, size_next_layer in zip(self.size_layers, size_next_layers):\n",
    "            n_weights = (size_next_layer * (size_layer + extra_item))\n",
    "            data_tmp = unrolled_data[0 : n_weights]\n",
    "            data_tmp = data_tmp.reshape(size_next_layer, (size_layer + extra_item), order = 'F')\n",
    "            rolled_list.append(data_tmp)\n",
    "            unrolled_data = np.delete(unrolled_data, np.s_[0:n_weights])\n",
    "        return rolled_list\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    "def plot_digits(X, y, num_digits_to_plot=3):\n",
    "    for i in range(num_digits_to_plot):\n",
    "        _image = X[i,:]\n",
    "        _label = y[i]\n",
    "\n",
    "        # 784 columns correspond to 28x28 image\n",
    "        plottable_image = np.reshape(_image, (28, 28))\n",
    "\n",
    "        # Plot the image\n",
    "        plt.figure()\n",
    "        plt.imshow(plottable_image, cmap='gray_r')\n",
    "        plt.title('Digit Label: {}'.format(_label))\n",
    "        plt.show()\n",
    "\n",
    "# Load data\n",
    "X, Y, labels, y = load_mnist()\n",
    "plot_digits(X, y, num_digits_to_plot=5)\n",
    "print('num examples by num of features', X.shape)\n",
    "\n",
    "tic = time.time()\n",
    "epochs = 17 \n",
    "\"\"\"enter the number of epochs for training \"EP\" \"\"\" \n",
    "loss = np.zeros([epochs,1])\n",
    "# Creating the MLP object initialize the weights\n",
    "mlp_classifier = Mlp(size_layers = [784, 777, 10], \n",
    "                     #\"\"\"enter the number of neurons in hidden layer \"NN\" \"\"\", 10], \n",
    "                         act_f   = 'relu',\n",
    "                         reg_lambda  = 0,\n",
    "                         bias_flag   = False)\n",
    "\n",
    "for ix in range(epochs):\n",
    "    mlp_classifier.train(X, Y, 17)\n",
    "                       #  \"\"\"Number of times the back-propagation algorithm is performed for each epoch \"BP\"  \"\"\")\n",
    "    Y_hat = mlp_classifier.predict(X)\n",
    "    # loss\n",
    "    loss[ix] = (0.5)*np.square(Y_hat - Y).mean()\n",
    "\n",
    "print(str(time.time() - tic) + ' s')\n",
    "        \n",
    "# Ploting loss vs epochs\n",
    "plt.figure()\n",
    "ix = np.arange(epochs)\n",
    "plt.plot(ix, loss)\n",
    "\n",
    "# Training Accuracy\n",
    "Y_hat = mlp_classifier.predict(X)\n",
    "y_tmp = np.argmax(Y_hat, axis=1)\n",
    "y_hat = labels[y_tmp]\n",
    "\n",
    "acc = np.mean(1 * (y_hat == y))\n",
    "print('Training Accuracy: ' + str(acc*100) + '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(data,target,n_hidden_layers=1,n_hidden_len = 2,n_iter=50):\n",
    "    network = {} \n",
    "    outputs = {}\n",
    "        \n",
    "    all_layers = n_hidden_layers + 1 #includes output layer\n",
    "    \n",
    "    outputs = np.zeros(n_hidden_len)\n",
    "    \n",
    "    print('all_layers , includes output layer', all_layers)\n",
    "        \n",
    "    data = data[:n_iter]\n",
    "    for idx, sample in enumerate(data):\n",
    "        for layer in range(1,all_layers+1,1):\n",
    "            for hidden_len in range(n_hidden_len):\n",
    "                #print('sample',sample,'idx',idx,'layer',layer+1,'layer_len',layer_len)\n",
    "                print('layer',layer,'Perceptron',hidden_len)\n",
    "                if layer == 1:\n",
    "                    P = Perceptron(42).fit(sample,target[idx])\n",
    "                    print('[O_]',P.O_)\n",
    "                    #key = str(layer) + ' ' + str(hidden_len)\n",
    "                    network[layer] = P\n",
    "                    outputs.append(P.O_)\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "                    #print('layer',layer)\n",
    "                    #print(outputs[layer])\n",
    "                    \n",
    "                    #network[layer+1] = Perceptron(42).fit(np.array(outputs),target[idx])\n",
    "                #propagate the inputs forward\n",
    "                #propagate_forward(layer,sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
