{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex p. 298\n",
    "#Info(D) = I(9,5)\n",
    "ID = -9/14 * np.log2(9/14) - 5/14*np.log2(5/14)\n",
    "ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_Age = (((- (2/5) * np.log2(2/5)) - ( (3/5) * np.log2(3/5))) * (5/14)) + 0 + (((- (2/5) * np.log2(2/5)) - ( (3/5) * np.log2(3/5))) * (5/14))\n",
    "ID_Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high - 4 \n",
    "#medium - 6\n",
    "#low - 4\n",
    "\n",
    "#4/14 * I(2,2) + 6/14 * I(4,2) + 4/14 I(3,1)\n",
    "high = (((- (2/4) * np.log2(2/4)) - ( (2/4) * np.log2(2/4))) * (4/14))\n",
    "medium = (((- (4/6) * np.log2(4/6)) - ( (2/6) * np.log2(2/6))) * (6/14))\n",
    "low = (((- (3/4) * np.log2(3/4)) - ( (1/4) * np.log2(1/4))) * (4/14))\n",
    "\n",
    "ID_income = high + medium + low\n",
    "display(ID_income)\n",
    "\n",
    "Gain_income = ID - ID_income\n",
    "Gain_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no - 7\n",
    "#yes- 7\n",
    "\n",
    "# no                   yes\n",
    "#7/14 * I(3,4) + 7/14 * I(6,1) \n",
    "no = (((- (3/7) * np.log2(3/7)) - ( (4/7) * np.log2(4/7))) * (7/14))\n",
    "yes = (((- (6/7) * np.log2(6/7)) - ( (1/7) * np.log2(1/7))) * (7/14))\n",
    "\n",
    "ID_student = no + yes\n",
    "ID_student\n",
    "\n",
    "Gain_student = ID - ID_student\n",
    "Gain_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fair - 8\n",
    "#exce- 6\n",
    "\n",
    "\n",
    "#8/14 * I(6,2) + 6/14 * I(3,3) \n",
    "fair = (((- (6/8) * np.log2(6/8)) - ( (2/8) * np.log2(2/8))) * (8/14))\n",
    "exce = (((- (3/6) * np.log2(3/6)) - ( (3/6) * np.log2(3/6))) * (6/14))\n",
    "\n",
    "ID_credit = fair + exce\n",
    "ID_credit\n",
    "\n",
    "Gain_credit = ID - ID_credit\n",
    "Gain_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex p. 299\n",
    "# + - 3,3\n",
    "\n",
    "Entropy = (((- (3/6) * np.log2(3/6)) - ( (3/6) * np.log2(3/6))))\n",
    "Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a2 gain\n",
    "#T- 4\n",
    "#F- 2\n",
    "\n",
    "\n",
    "#4/6 * I(2,2) + 2/6 * I(1,1) \n",
    "T = 4/6 * 1\n",
    "F = 2/6 * 1\n",
    "\n",
    "ID_a2 = T + F\n",
    "display(ID_a2)\n",
    "\n",
    "Gain_a2 = Entropy - ID_a2\n",
    "Gain_a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex 300 - entropy\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def gain_func(entropy_root,X,y):\n",
    "        \n",
    "    if (len(X.columns) == 0):\n",
    "        return\n",
    "    \n",
    "    if entropy_root == None:\n",
    "        #handle root\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "                                \n",
    "        len_arr = y.shape[0]\n",
    "        count = 0\n",
    "        entropy_root = np.float(0)\n",
    "    \n",
    "        for i in counts:\n",
    "            count += i\n",
    "            entropy_root += (- (i/len_arr) * np.log2(i/len_arr))\n",
    "            \n",
    "        entropy_root = entropy_root * (count/len_arr)\n",
    "        gain = 0\n",
    "        \n",
    "        print('entropy_root',entropy_root)\n",
    "    \n",
    "    else:\n",
    "        gains = []\n",
    "        entropies = []\n",
    "        for feature in X.columns:\n",
    "            uniques, indexes,counts = np.unique(X[feature], return_counts=True,return_index=True)\n",
    "            #print('feature',feature,'uniques', uniques,'counts',counts)\n",
    "            feature_entropy = 0\n",
    "            \n",
    "            for value in uniques:\n",
    "                idx_values = X[feature].index[X[feature] == value].tolist()\n",
    "                yes_count = sum(map(lambda x : x == 'yes', y[idx_values]))\n",
    "                no_count = sum(map(lambda x : x == 'no', y[idx_values]))\n",
    "                #print('value',value,'idx_values',idx_values,'yes_count',yes_count,'no_count',no_count)\n",
    "                \n",
    "                targets_by_feature_count = yes_count + no_count\n",
    "                feature_count = X[feature].size\n",
    "                \n",
    "                if (no_count != 0 and yes_count != 0):\n",
    "                    feature_entropy += (((- (yes_count/targets_by_feature_count) * np.log2(yes_count/targets_by_feature_count)) -\n",
    "                                    ( (no_count/targets_by_feature_count) * np.log2(no_count/targets_by_feature_count))) * \n",
    "                                   (targets_by_feature_count/feature_count))\n",
    "                            \n",
    "            \n",
    "            gain = entropy_root - feature_entropy\n",
    "            gains.append(gain)\n",
    "            entropies.append(feature_entropy)\n",
    "       \n",
    "            \n",
    "       \n",
    "        feature_max = X.columns[np.argmax(gains)] \n",
    "        print('feature_max_gain [',feature_max,']')\n",
    "        print('entropies find min',entropies)\n",
    "        uniques, indexes,counts = np.unique(X[feature_max], return_counts=True,return_index=True)\n",
    "        #print('feature_max',feature_max,'uniques', uniques,'counts',counts)\n",
    "        for value in uniques:\n",
    "            idx_values = X[feature_max].index[X[feature_max] == value].tolist()\n",
    "            yes_count = sum(map(lambda x : x == 'yes', y[idx_values]))\n",
    "            no_count = sum(map(lambda x : x == 'no', y[idx_values]))\n",
    "            #print('value',value,'idx_values',idx_values,'yes_count',yes_count,'no_count',no_count)\n",
    "            if (yes_count==0):\n",
    "                print('feature',feature,'value', value,'[NO]')\n",
    "            if (no_count==0):\n",
    "                print('feature',feature,'value', value,'[YES]')\n",
    "            \n",
    "       \n",
    "\n",
    "    \n",
    "d = {'outlook':['Sunny','Sunny','Overcast','Rain','Rain','Rain','Overcast','Sunny','Sunny','Rain','Sunny','Overcast','Overcast','Rain'],\n",
    "    'temp':['hot','hot','hot','m','c','c','c','m','c','m','m','m','hot','m'],\n",
    "    'humidity':['High','High','High','High','Nomral','Nomral','Nomral','High','Nomral','Nomral','Nomral','High','Nomral','High'],\n",
    "    'wind':['Weak','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Weak','Weak','Strong','Strong','Weak','Strong'],\n",
    "    'play':['no','no','yes','yes','yes','no','yes','no','yes','yes','yes','yes','yes','no']}\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "X = df.drop('play',axis=1)\n",
    "y = df.play\n",
    "\n",
    "print('************* ROOT *************')\n",
    "gain_func(None,X,y)\n",
    "\n",
    "print('************* FIRST LEVEL *************')\n",
    "gain_func(0.9402859586706311,X,y)\n",
    "\n",
    "print('************* outlook - sunny *************')\n",
    "X_part = X.drop('outlook',axis=1)\n",
    "#print(X_part[X_part.index.isin([0, 1, 7, 8, 10])] ,y[y.index.isin([0, 1, 7, 8, 10])])\n",
    "gain_func(0.6935361,X_part[X_part.index.isin([0, 1, 7, 8, 10])] ,y[y.index.isin([0, 1, 7, 8, 10])])\n",
    "\n",
    "print('************* outlook - sunny - humidity  - High *************')\n",
    "X_part_2 = X_part.drop('humidity',axis=1)\n",
    "#print(X_part[X_part_2.index.isin([0, 1, 7])] ,y[y.index.isin([0, 1, 7])])\n",
    "gain_func(0.0,X_part[X_part_2.index.isin([0, 1, 7])] ,y[y.index.isin([0, 1, 7])])\n",
    "\n",
    "\n",
    "print('************* outlook - rain *************')\n",
    "X_part = X.drop('outlook',axis=1)\n",
    "#print(X_part[X_part.index.isin([3, 4, 5, 9, 13])] ,y[y.index.isin([3, 4, 5, 9, 13])])\n",
    "gain_func(0.6935361,X_part[X_part.index.isin([3, 4, 5, 9, 13])] ,y[y.index.isin([3, 4, 5, 9, 13])])\n",
    "\n",
    "#for g in gain_func(None,X,y):\n",
    " #   print('********************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play No 5\n",
    "#Play Yes 9\n",
    "\n",
    "Entropy = (((- (5/14) * np.log2(5/14)) - ( (9/14) * np.log2(9/14))))\n",
    "Entropy\n",
    "\n",
    "#Outlook\n",
    "#Sunny - 5\n",
    "#Overcast - 4\n",
    "#Rain - 5\n",
    "\n",
    "#5/14 * I(2,3) + 4/14 * I(4,0) + 5/14 I(3,2)\n",
    "Sunny = (((- (2/5) * np.log2(2/5)) - ( (3/5) * np.log2(3/5))) * (5/14))\n",
    "Overcast = 0\n",
    "Rain = (((- (3/5) * np.log2(3/5)) - ( (2/5) * np.log2(2/5))) * (5/14))\n",
    "print('Rain',Rain)\n",
    "Outlook = Sunny + Overcast + Rain\n",
    "display(Outlook)\n",
    "\n",
    "Gain = Entropy - Outlook\n",
    "Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlook\n",
    "#Sunny - 5\n",
    "\n",
    "# temp \n",
    "#   hot = 2 (0,2)\n",
    "#   mild - 2 (1,1)\n",
    "#   cool - 1 (1,0)\n",
    "\n",
    "Sunny_Temp = (((- (1/2) * np.log2(1/2)) - ( (1/2) * np.log2(1/2))) * (2/5))\n",
    "Sunny_Temp\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* ROOT *************\n",
      "uniques ['no' 'yes'] counts [5 5]\n",
      "gini_root 0.5\n",
      "************* FIRST LEVEL *************\n",
      "feature Color uniques ['Red' 'Yellow'] counts [5 5]\n",
      "value Red idx_values [0, 1, 2, 8, 9] yes_count 3 no_count 2\n",
      "targets_by_feature_count 5 feature_count 10\n",
      "value Yellow idx_values [3, 4, 5, 6, 7] yes_count 2 no_count 3\n",
      "targets_by_feature_count 5 feature_count 10\n",
      "feature Type uniques ['SUV' 'Sports'] counts [4 6]\n",
      "value SUV idx_values [5, 6, 7, 8] yes_count 1 no_count 3\n",
      "targets_by_feature_count 4 feature_count 10\n",
      "value Sports idx_values [0, 1, 2, 3, 4, 9] yes_count 4 no_count 2\n",
      "targets_by_feature_count 6 feature_count 10\n",
      "feature Origin uniques ['Domestic' 'Imported'] counts [5 5]\n",
      "value Domestic idx_values [0, 1, 2, 3, 7] yes_count 2 no_count 3\n",
      "targets_by_feature_count 5 feature_count 10\n",
      "value Imported idx_values [4, 5, 6, 8, 9] yes_count 3 no_count 2\n",
      "targets_by_feature_count 5 feature_count 10\n",
      "feature_max_gain [ Type ]\n",
      "ginies find min [0.48, 0.4166666666666667, 0.48]\n",
      "************* outlook - sunny *************\n",
      "************* outlook - sunny - humidity  - High *************\n",
      "************* outlook - rain *************\n"
     ]
    }
   ],
   "source": [
    "#Ex 305 - gini\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "    \n",
    "\n",
    "def gain_gini_func(gini_root,X,y):\n",
    "        \n",
    "    if (len(X.columns) == 0):\n",
    "        return\n",
    "    \n",
    "    if gini_root == None:\n",
    "        #handle root\n",
    "        uniques, counts = np.unique(y, return_counts=True)\n",
    "        print('uniques',uniques,'counts',counts)\n",
    "        \n",
    "        len_arr = y.shape[0]\n",
    "        count = 0\n",
    "        gini_root = np.float(0)\n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in counts:\n",
    "            count += i\n",
    "            gini_root += (i/len_arr) ** 2\n",
    "            \n",
    "        gini_root = 1 - gini_root\n",
    "        gain = 0\n",
    "        \n",
    "        print('gini_root',gini_root)\n",
    "    \n",
    "    else:\n",
    "        gains = []\n",
    "        ginies = []\n",
    "        for feature in X.columns:\n",
    "            uniques, indexes,counts = np.unique(X[feature], return_counts=True,return_index=True)\n",
    "            print('feature',feature,'uniques', uniques,'counts',counts)\n",
    "            feature_gini = 0\n",
    "            \n",
    "            for value in uniques:\n",
    "                idx_values = X[feature].index[X[feature] == value].tolist()\n",
    "                yes_count = sum(map(lambda x : x == 'yes', y[idx_values]))\n",
    "                no_count = sum(map(lambda x : x == 'no', y[idx_values]))\n",
    "                print('value',value,'idx_values',idx_values,'yes_count',yes_count,'no_count',no_count)\n",
    "                \n",
    "                targets_by_feature_count = yes_count + no_count\n",
    "                feature_count = X[feature].size\n",
    "                \n",
    "                print('targets_by_feature_count',targets_by_feature_count,'feature_count',feature_count)\n",
    "                \n",
    "                if (no_count != 0 and yes_count != 0):\n",
    "                    feature_gini += ((targets_by_feature_count/feature_count) * (1 - (yes_count/targets_by_feature_count)**2 - (no_count/targets_by_feature_count)**2  )) \n",
    "                    \n",
    "                                      \n",
    "                    \n",
    "                            \n",
    "            \n",
    "            gain = gini_root - feature_gini\n",
    "            gains.append(gain)\n",
    "            ginies.append(feature_gini)\n",
    "       \n",
    "            \n",
    "       \n",
    "        feature_max = X.columns[np.argmax(gains)] \n",
    "        print('feature_max_gain [',feature_max,']')\n",
    "        print('ginies find min',ginies)\n",
    "        uniques, indexes,counts = np.unique(X[feature_max], return_counts=True,return_index=True)\n",
    "        #print('feature_max',feature_max,'uniques', uniques,'counts',counts)\n",
    "        for value in uniques:\n",
    "            idx_values = X[feature_max].index[X[feature_max] == value].tolist()\n",
    "            yes_count = sum(map(lambda x : x == 'yes', y[idx_values]))\n",
    "            no_count = sum(map(lambda x : x == 'no', y[idx_values]))\n",
    "            #print('value',value,'idx_values',idx_values,'yes_count',yes_count,'no_count',no_count)\n",
    "            if (yes_count==0):\n",
    "                print('feature',feature,'value', value,'[NO]')\n",
    "            if (no_count==0):\n",
    "                print('feature',feature,'value', value,'[YES]')\n",
    "            \n",
    "       \n",
    "\n",
    "    \n",
    "d = {'Color':['Red','Red','Red','Yellow','Yellow','Yellow','Yellow','Yellow','Red','Red'],\n",
    "    'Type':['Sports','Sports','Sports','Sports','Sports','SUV','SUV','SUV','SUV','Sports'],\n",
    "    'Origin':['Domestic','Domestic','Domestic','Domestic','Imported','Imported','Imported','Domestic','Imported','Imported'],\n",
    "    'Stolen':['yes','no','yes','no','yes','no','yes','no','no','yes']\n",
    "    }\n",
    "df = pd.DataFrame(d)\n",
    "\n",
    "X = df.drop('Stolen',axis=1)\n",
    "y = df.Stolen\n",
    "\n",
    "print('************* ROOT *************')\n",
    "gain_gini_func(None,X,y)\n",
    "\n",
    "print('************* FIRST LEVEL *************')\n",
    "gain_gini_func(0.5,X,y)\n",
    "\n",
    "print('************* outlook - sunny *************')\n",
    "#X_part = X.drop('outlook',axis=1)\n",
    "#print(X_part[X_part.index.isin([0, 1, 7, 8, 10])] ,y[y.index.isin([0, 1, 7, 8, 10])])\n",
    "#gain_func(0.6935361,X_part[X_part.index.isin([0, 1, 7, 8, 10])] ,y[y.index.isin([0, 1, 7, 8, 10])])\n",
    "\n",
    "print('************* outlook - sunny - humidity  - High *************')\n",
    "#X_part_2 = X_part.drop('humidity',axis=1)\n",
    "#print(X_part[X_part_2.index.isin([0, 1, 7])] ,y[y.index.isin([0, 1, 7])])\n",
    "#gain_func(0.0,X_part[X_part_2.index.isin([0, 1, 7])] ,y[y.index.isin([0, 1, 7])])\n",
    "\n",
    "\n",
    "print('************* outlook - rain *************')\n",
    "#X_part = X.drop('outlook',axis=1)\n",
    "#print(X_part[X_part.index.isin([3, 4, 5, 9, 13])] ,y[y.index.isin([3, 4, 5, 9, 13])])\n",
    "#gain_func(0.6935361,X_part[X_part.index.isin([3, 4, 5, 9, 13])] ,y[y.index.isin([3, 4, 5, 9, 13])])\n",
    "\n",
    "#for g in gain_func(None,X,y):\n",
    " #   print('********************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_ {'criterion': 'gini', 'max_leaf_nodes': 28, 'splitter': 'best'}\n",
      "0.77175 {'criterion': 'gini', 'max_leaf_nodes': 2, 'splitter': 'best'}\n",
      "0.66475 {'criterion': 'gini', 'max_leaf_nodes': 2, 'splitter': 'random'}\n",
      "0.8525 {'criterion': 'gini', 'max_leaf_nodes': 4, 'splitter': 'best'}\n",
      "0.748 {'criterion': 'gini', 'max_leaf_nodes': 4, 'splitter': 'random'}\n",
      "0.8525 {'criterion': 'gini', 'max_leaf_nodes': 6, 'splitter': 'best'}\n",
      "0.7835 {'criterion': 'gini', 'max_leaf_nodes': 6, 'splitter': 'random'}\n",
      "0.8525 {'criterion': 'gini', 'max_leaf_nodes': 8, 'splitter': 'best'}\n",
      "0.79825 {'criterion': 'gini', 'max_leaf_nodes': 8, 'splitter': 'random'}\n",
      "0.85125 {'criterion': 'gini', 'max_leaf_nodes': 10, 'splitter': 'best'}\n",
      "0.82125 {'criterion': 'gini', 'max_leaf_nodes': 10, 'splitter': 'random'}\n",
      "0.853 {'criterion': 'gini', 'max_leaf_nodes': 12, 'splitter': 'best'}\n",
      "0.815 {'criterion': 'gini', 'max_leaf_nodes': 12, 'splitter': 'random'}\n",
      "0.85325 {'criterion': 'gini', 'max_leaf_nodes': 14, 'splitter': 'best'}\n",
      "0.8225 {'criterion': 'gini', 'max_leaf_nodes': 14, 'splitter': 'random'}\n",
      "0.859 {'criterion': 'gini', 'max_leaf_nodes': 16, 'splitter': 'best'}\n",
      "0.84125 {'criterion': 'gini', 'max_leaf_nodes': 16, 'splitter': 'random'}\n",
      "0.86 {'criterion': 'gini', 'max_leaf_nodes': 18, 'splitter': 'best'}\n",
      "0.826 {'criterion': 'gini', 'max_leaf_nodes': 18, 'splitter': 'random'}\n",
      "0.8595 {'criterion': 'gini', 'max_leaf_nodes': 20, 'splitter': 'best'}\n",
      "0.8375 {'criterion': 'gini', 'max_leaf_nodes': 20, 'splitter': 'random'}\n",
      "0.8585 {'criterion': 'gini', 'max_leaf_nodes': 22, 'splitter': 'best'}\n",
      "0.8315 {'criterion': 'gini', 'max_leaf_nodes': 22, 'splitter': 'random'}\n",
      "0.8585 {'criterion': 'gini', 'max_leaf_nodes': 24, 'splitter': 'best'}\n",
      "0.82775 {'criterion': 'gini', 'max_leaf_nodes': 24, 'splitter': 'random'}\n",
      "0.86 {'criterion': 'gini', 'max_leaf_nodes': 26, 'splitter': 'best'}\n",
      "0.846 {'criterion': 'gini', 'max_leaf_nodes': 26, 'splitter': 'random'}\n",
      "0.86175 {'criterion': 'gini', 'max_leaf_nodes': 28, 'splitter': 'best'}\n",
      "0.83975 {'criterion': 'gini', 'max_leaf_nodes': 28, 'splitter': 'random'}\n",
      "0.86 {'criterion': 'gini', 'max_leaf_nodes': 30, 'splitter': 'best'}\n",
      "0.84975 {'criterion': 'gini', 'max_leaf_nodes': 30, 'splitter': 'random'}\n",
      "0.86025 {'criterion': 'gini', 'max_leaf_nodes': 32, 'splitter': 'best'}\n",
      "0.848 {'criterion': 'gini', 'max_leaf_nodes': 32, 'splitter': 'random'}\n",
      "0.8595 {'criterion': 'gini', 'max_leaf_nodes': 34, 'splitter': 'best'}\n",
      "0.8565 {'criterion': 'gini', 'max_leaf_nodes': 34, 'splitter': 'random'}\n",
      "0.85925 {'criterion': 'gini', 'max_leaf_nodes': 36, 'splitter': 'best'}\n",
      "0.8485 {'criterion': 'gini', 'max_leaf_nodes': 36, 'splitter': 'random'}\n",
      "0.859 {'criterion': 'gini', 'max_leaf_nodes': 38, 'splitter': 'best'}\n",
      "0.849 {'criterion': 'gini', 'max_leaf_nodes': 38, 'splitter': 'random'}\n",
      "0.85825 {'criterion': 'gini', 'max_leaf_nodes': 40, 'splitter': 'best'}\n",
      "0.8455 {'criterion': 'gini', 'max_leaf_nodes': 40, 'splitter': 'random'}\n",
      "0.857 {'criterion': 'gini', 'max_leaf_nodes': 42, 'splitter': 'best'}\n",
      "0.843 {'criterion': 'gini', 'max_leaf_nodes': 42, 'splitter': 'random'}\n",
      "0.85525 {'criterion': 'gini', 'max_leaf_nodes': 44, 'splitter': 'best'}\n",
      "0.852 {'criterion': 'gini', 'max_leaf_nodes': 44, 'splitter': 'random'}\n",
      "0.8565 {'criterion': 'gini', 'max_leaf_nodes': 46, 'splitter': 'best'}\n",
      "0.83725 {'criterion': 'gini', 'max_leaf_nodes': 46, 'splitter': 'random'}\n",
      "0.85575 {'criterion': 'gini', 'max_leaf_nodes': 48, 'splitter': 'best'}\n",
      "0.8475 {'criterion': 'gini', 'max_leaf_nodes': 48, 'splitter': 'random'}\n",
      "0.769 {'criterion': 'entropy', 'max_leaf_nodes': 2, 'splitter': 'best'}\n",
      "0.62975 {'criterion': 'entropy', 'max_leaf_nodes': 2, 'splitter': 'random'}\n",
      "0.85075 {'criterion': 'entropy', 'max_leaf_nodes': 4, 'splitter': 'best'}\n",
      "0.74825 {'criterion': 'entropy', 'max_leaf_nodes': 4, 'splitter': 'random'}\n",
      "0.85075 {'criterion': 'entropy', 'max_leaf_nodes': 6, 'splitter': 'best'}\n",
      "0.79 {'criterion': 'entropy', 'max_leaf_nodes': 6, 'splitter': 'random'}\n",
      "0.85075 {'criterion': 'entropy', 'max_leaf_nodes': 8, 'splitter': 'best'}\n",
      "0.79625 {'criterion': 'entropy', 'max_leaf_nodes': 8, 'splitter': 'random'}\n",
      "0.85075 {'criterion': 'entropy', 'max_leaf_nodes': 10, 'splitter': 'best'}\n",
      "0.79375 {'criterion': 'entropy', 'max_leaf_nodes': 10, 'splitter': 'random'}\n",
      "0.85075 {'criterion': 'entropy', 'max_leaf_nodes': 12, 'splitter': 'best'}\n",
      "0.819 {'criterion': 'entropy', 'max_leaf_nodes': 12, 'splitter': 'random'}\n",
      "0.85175 {'criterion': 'entropy', 'max_leaf_nodes': 14, 'splitter': 'best'}\n",
      "0.8275 {'criterion': 'entropy', 'max_leaf_nodes': 14, 'splitter': 'random'}\n",
      "0.85325 {'criterion': 'entropy', 'max_leaf_nodes': 16, 'splitter': 'best'}\n",
      "0.82875 {'criterion': 'entropy', 'max_leaf_nodes': 16, 'splitter': 'random'}\n",
      "0.851 {'criterion': 'entropy', 'max_leaf_nodes': 18, 'splitter': 'best'}\n",
      "0.8285 {'criterion': 'entropy', 'max_leaf_nodes': 18, 'splitter': 'random'}\n",
      "0.8515 {'criterion': 'entropy', 'max_leaf_nodes': 20, 'splitter': 'best'}\n",
      "0.83675 {'criterion': 'entropy', 'max_leaf_nodes': 20, 'splitter': 'random'}\n",
      "0.85325 {'criterion': 'entropy', 'max_leaf_nodes': 22, 'splitter': 'best'}\n",
      "0.836 {'criterion': 'entropy', 'max_leaf_nodes': 22, 'splitter': 'random'}\n",
      "0.853 {'criterion': 'entropy', 'max_leaf_nodes': 24, 'splitter': 'best'}\n",
      "0.842 {'criterion': 'entropy', 'max_leaf_nodes': 24, 'splitter': 'random'}\n",
      "0.85375 {'criterion': 'entropy', 'max_leaf_nodes': 26, 'splitter': 'best'}\n",
      "0.84875 {'criterion': 'entropy', 'max_leaf_nodes': 26, 'splitter': 'random'}\n",
      "0.8535 {'criterion': 'entropy', 'max_leaf_nodes': 28, 'splitter': 'best'}\n",
      "0.84525 {'criterion': 'entropy', 'max_leaf_nodes': 28, 'splitter': 'random'}\n",
      "0.85425 {'criterion': 'entropy', 'max_leaf_nodes': 30, 'splitter': 'best'}\n",
      "0.83775 {'criterion': 'entropy', 'max_leaf_nodes': 30, 'splitter': 'random'}\n",
      "0.85375 {'criterion': 'entropy', 'max_leaf_nodes': 32, 'splitter': 'best'}\n",
      "0.85425 {'criterion': 'entropy', 'max_leaf_nodes': 32, 'splitter': 'random'}\n",
      "0.85375 {'criterion': 'entropy', 'max_leaf_nodes': 34, 'splitter': 'best'}\n",
      "0.847 {'criterion': 'entropy', 'max_leaf_nodes': 34, 'splitter': 'random'}\n",
      "0.8525 {'criterion': 'entropy', 'max_leaf_nodes': 36, 'splitter': 'best'}\n",
      "0.83775 {'criterion': 'entropy', 'max_leaf_nodes': 36, 'splitter': 'random'}\n",
      "0.85225 {'criterion': 'entropy', 'max_leaf_nodes': 38, 'splitter': 'best'}\n",
      "0.847 {'criterion': 'entropy', 'max_leaf_nodes': 38, 'splitter': 'random'}\n",
      "0.8545 {'criterion': 'entropy', 'max_leaf_nodes': 40, 'splitter': 'best'}\n",
      "0.838 {'criterion': 'entropy', 'max_leaf_nodes': 40, 'splitter': 'random'}\n",
      "0.85425 {'criterion': 'entropy', 'max_leaf_nodes': 42, 'splitter': 'best'}\n",
      "0.83575 {'criterion': 'entropy', 'max_leaf_nodes': 42, 'splitter': 'random'}\n",
      "0.854 {'criterion': 'entropy', 'max_leaf_nodes': 44, 'splitter': 'best'}\n",
      "0.84925 {'criterion': 'entropy', 'max_leaf_nodes': 44, 'splitter': 'random'}\n",
      "0.85375 {'criterion': 'entropy', 'max_leaf_nodes': 46, 'splitter': 'best'}\n",
      "0.83475 {'criterion': 'entropy', 'max_leaf_nodes': 46, 'splitter': 'random'}\n",
      "0.85275 {'criterion': 'entropy', 'max_leaf_nodes': 48, 'splitter': 'best'}\n",
      "0.84 {'criterion': 'entropy', 'max_leaf_nodes': 48, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X, y = make_moons(n_samples=20000,noise=0.4)\n",
    "\n",
    "X_train,X_test,y_train ,y_test = train_test_split(X,y,random_state = 42,train_size =0.2)\n",
    "X_train,X_test ,y_train ,y_test\n",
    "\n",
    "\n",
    "dtc_cls = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = {'max_leaf_nodes':np.arange(2,50,2),\n",
    "             'criterion' :['gini','entropy']\n",
    "             'splitter' : ['best','random']}\n",
    "             \n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(dtc_cls,param_grid,cv=10)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print('best_params_',grid_search.best_params_)\n",
    "\n",
    "results = grid_search.cv_results_\n",
    "for mean_score,params in zip(results['mean_test_score'],results['params']):\n",
    "    print(mean_score,params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.85321674, 0.85571518, 0.85821362, 0.861875  , 0.869375  ,\n",
       "        0.86      , 0.85375   , 0.86616635, 0.86429018, 0.86741714]),\n",
       " 0.8610019205085628,\n",
       " 0.005472086169917997)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection  import cross_val_score\n",
    "dtc_cls_best = DecisionTreeClassifier(max_leaf_nodes=28,criterion='gini',splitter='best')\n",
    "scores = cross_val_score(dtc_cls_best,X_test,y_test,cv =10)\n",
    "scores,scores.mean(),scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"Generic tree node.\"\n",
    "    def __init__(self, name='root', children=None):\n",
    "        self.name = name\n",
    "        self.children = []\n",
    "        if children is not None:\n",
    "            for child in children:\n",
    "                self.add_child(child)\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    def add_child(self, node):\n",
    "        assert isinstance(node, Tree)\n",
    "        self.children.append(node)\n",
    "#    *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from pprint import pprint  \n",
    "#Import the dataset and define the feature as well as the target datasets / columns#  \n",
    "dataset = pd.read_csv('zoo.csv',  \n",
    "                      names=['animal_name','hair','feathers','eggs','milk',  \n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',  \n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals  \n",
    "#We drop the animal names since this is not a good feature to split the data on  \n",
    "dataset=dataset.drop('animal_name',axis=1)  \n",
    "  \n",
    "def entropy(target_col):  \n",
    "    \n",
    "    elements,counts = np.unique(target_col,return_counts = True)  \n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])  \n",
    "    return entropy  \n",
    "  \n",
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):  \n",
    "         \n",
    "    #Calculate the entropy of the total dataset  \n",
    "    total_entropy = entropy(data[target_name])  \n",
    "      \n",
    "    ##Calculate the entropy of the dataset  \n",
    "      \n",
    "    #Calculate the values and the corresponding counts for the split attribute   \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)  \n",
    "      \n",
    "    #Calculate the weighted entropy  \n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])  \n",
    "      \n",
    "    #Calculate the information gain  \n",
    "    Information_Gain = total_entropy - Weighted_Entropy  \n",
    "    return Information_Gain  \n",
    "  \n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):  \n",
    "  \n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#  \n",
    "      \n",
    "    #If all target_values have the same value, return this value  \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:  \n",
    "        return np.unique(data[target_attribute_name])[0]  \n",
    "      \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset  \n",
    "    elif len(data)==0:  \n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(origin\n",
    "Introduction\n",
    "aldata[target_attribute_name],return_counts=True)[1])]  \n",
    "      \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that  \n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence  \n",
    "    #the mode target feature value is stored in the parent_node_class variable.  \n",
    "      \n",
    "    elif len(features) ==0:  \n",
    "        return parent_node_class  \n",
    "      \n",
    "    #If none of the above holds true, grow the tree!  \n",
    "      \n",
    "    else:  \n",
    "        #Set the default value for this node --> The mode target feature value of the current node  \n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]  \n",
    "          \n",
    "        #Select the feature which best splits the dataset  \n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset  \n",
    "        best_feature_index = np.argmax(item_values)  \n",
    "        best_feature = features[best_feature_index]  \n",
    "          \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information  \n",
    "        #gain in the first run  \n",
    "        tree = {best_feature:{}}  \n",
    "          \n",
    "          \n",
    "        #Remove the feature with the best inforamtion gain from the feature space  \n",
    "        features = [i for i in features if i != best_feature]  \n",
    "          \n",
    "        #Grow a branch under the root node for each possible value of the root node feature  \n",
    "          \n",
    "        for value in np.unique(data[best_feature]):  \n",
    "            value = value  \n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets  \n",
    "            sub_data = data.where(data[best_feature] == value).dropna()  \n",
    "              \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!  \n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)  \n",
    "              \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node  \n",
    "            tree[best_feature][value] = subtree  \n",
    "              \n",
    "        return(tree)      \n",
    "                  \n",
    "def predict(query,tree,default = 1):  \n",
    "    \n",
    "    for key in list(query.keys()):  \n",
    "        if key in list(tree.keys()):  \n",
    "            \n",
    "            try:  \n",
    "                result = tree[key][query[key]]   \n",
    "            except:  \n",
    "                return default  \n",
    "    \n",
    "            result = tree[key][query[key]]  \n",
    "            \n",
    "            if isinstance(result,dict):  \n",
    "                return predict(query,result)  \n",
    "            else:  \n",
    "                return result  \n",
    "  \n",
    "def train_test_split(dataset):  \n",
    "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index  \n",
    "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes  \n",
    "    testing_data = dataset.iloc[80:].reset_index(drop=True)  \n",
    "    return training_data,testing_data  \n",
    "  \n",
    "training_data = train_test_split(dataset)[0]  \n",
    "testing_data = train_test_split(dataset)[1]   \n",
    "  \n",
    "def test(data,tree):  \n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and   \n",
    "    #convert it to a dictionary  \n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")  \n",
    "      \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored  \n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"])   \n",
    "      \n",
    "    #Calculate the prediction accuracy  \n",
    "    for i in range(len(data)):  \n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0)   \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')  \n",
    "      \n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])  \n",
    "pprint(tree)  \n",
    "test(testing_data,tree) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth = 6, depth = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.depth = depth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        if self.depth <= self.max_depth: print(f\"processing at Depth: {self.depth}\")\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.independent = self.data.columns.tolist()\n",
    "        self.independent.remove(target)\n",
    "        if self.depth <= self.max_depth:\n",
    "            self.__validate_data()\n",
    "            self.impurity_score = self.__calculate_impurity_score(self.data[self.target])\n",
    "            self.criteria, self.split_feature, self.information_gain = self.__find_best_split()\n",
    "            if self.criteria is not None and self.information_gain > 0: self.__create_branches()\n",
    "        else: \n",
    "            print(\"Stopping splitting as Max depth reached\")\n",
    "    \n",
    "    def __create_branches(self):\n",
    "        self.left = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        self.right = DecisionTree(max_depth = self.max_depth, \n",
    "                                 depth = self.depth + 1)\n",
    "        left_rows = self.data[self.data[self.split_feature] <= self.criteria] \n",
    "        right_rows = self.data[self.data[self.split_feature] > self.criteria] \n",
    "        self.left.fit(data = left_rows, target = self.target)\n",
    "        self.right.fit(data = right_rows, target = self.target)\n",
    "    \n",
    "    def __calculate_impurity_score(self, data):\n",
    "       if data is None or data.empty: return 0\n",
    "       p_i, _ = data.value_counts().apply(lambda x: x/len(data)).tolist() \n",
    "       return p_i * (1 - p_i) * 2\n",
    "    \n",
    "    def __find_best_split(self):\n",
    "        best_split = {}\n",
    "        for col in self.independent:\n",
    "            information_gain, split = self.__find_best_split_for_column(col)\n",
    "            if split is None: continue\n",
    "            if not best_split or best_split[\"information_gain\"] < information_gain:\n",
    "                best_split = {\"split\": split, \"col\": col, \"information_gain\": information_gain}\n",
    "\n",
    "        return best_split.get(\"split\"), best_split.get(\"col\"), best_split.get(\"information_gain\")\n",
    "\n",
    "    def __find_best_split_for_column(self, col):\n",
    "        x = self.data[col]\n",
    "        unique_values = x.unique()\n",
    "        if len(unique_values) == 1: return None, None\n",
    "        information_gain = None\n",
    "        split = None\n",
    "        for val in unique_values:\n",
    "            left = x <= val\n",
    "            right = x > val\n",
    "            left_data = self.data[left]\n",
    "            right_data = self.data[right]\n",
    "            left_impurity = self.__calculate_impurity_score(left_data[self.target])\n",
    "            right_impurity = self.__calculate_impurity_score(right_data[self.target])\n",
    "            score = self.__calculate_information_gain(left_count = len(left_data),\n",
    "                                                      left_impurity = left_impurity,\n",
    "                                                      right_count = len(right_data),\n",
    "                                                      right_impurity = right_impurity)\n",
    "            if information_gain is None or score > information_gain: \n",
    "                information_gain = score \n",
    "                split = val\n",
    "        return information_gain, split\n",
    "    \n",
    "    def __calculate_information_gain(self, left_count, left_impurity, right_count, right_impurity):\n",
    "        return self.impurity_score - ((left_count/len(self.data)) * left_impurity + \\\n",
    "                                      (right_count/len(self.data)) * right_impurity)\n",
    "\n",
    "    def predict(self, data):\n",
    "        return np.array([self.__flow_data_thru_tree(row) for _, row in data.iterrows()])\n",
    "\n",
    "    def __validate_data(self):\n",
    "        non_numeric_columns = self.data[self.independent].select_dtypes(include=['category', 'object', 'bool']).columns.tolist()\n",
    "        if(len(set(self.independent).intersection(set(non_numeric_columns))) != 0):\n",
    "            raise RuntimeError(\"Not all columns are numeric\")\n",
    "        \n",
    "        self.data[self.target] = self.data[self.target].astype(\"category\")\n",
    "        if(len(self.data[self.target].cat.categories) != 2):\n",
    "            raise RuntimeError(\"Implementation is only for Binary Classification\")\n",
    "\n",
    "    def __flow_data_thru_tree(self, row):\n",
    "        if self.is_leaf_node: return self.probability\n",
    "        tree = self.left if row[self.split_feature] <= self.criteria else self.right\n",
    "        return tree.__flow_data_thru_tree(row)\n",
    "        \n",
    "    @property\n",
    "    def is_leaf_node(self): return self.left is None\n",
    "\n",
    "    @property\n",
    "    def probability(self): \n",
    "        return self.data[self.target].value_counts().apply(lambda x: x/len(self.data)).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
