{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "designed with the following guiding principles:\n",
    "1. consistency עקביות- all objects share a common interface drawn from limited set of methods\n",
    "2. Inspection -אפשרות בדיקה all the estimator's hyperparameters and learned parameters are accessible directly via public instance variables.\n",
    "3. Nonproliferation of classes - DS are represented as numpy arrays or spicy sparse metrics , instead of homemade classes\n",
    "4. Composition - Many machine learning tasks can be expressed as sequesnces of more fundamental algorithms, and scikit learn makes use of the wherever possible\n",
    "5. Sensible defaults - ערכי ברירת מחדל הגיוניים scikit learn provides reasonable default values for most parameters , making it easy to create a baseline working system quickly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn'a main objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator** (build the model)\n",
    "is an any object that can estimate some parameters based on a dataset\n",
    "all estimators implements the **fit()** method, which is used to learn the parameters of a model\n",
    "Any other parameters needed to guide the estimation process is considered a hyperparameter, and is generally set via a constructor parameter. this command causes a number internal computations to take place, and their results are stored in model specific attributes that we can explore. by convension, all model parameters that were learned during fit() process have trailing underscores. \n",
    "\n",
    "i.e LinearRegression class is an estimator. its learned parameters \"coef\\_\" and \"intercept\\_\"\n",
    "\n",
    "**Transformer** (i.e handle missing values)\n",
    "estimator that can also transform a dataset\n",
    "the trasnformation is performed by the **transform()** that takes the dataset to transform and returns the transformed dataset\n",
    "the transformation generally reliles on the learned parameters from the fit method\n",
    "all transformers also have a convenience method called **fit_transform()** (calling fit and then transform)\n",
    "i.e StandardScalar class is transformer\n",
    "\n",
    "**Predictors** \n",
    "estimator that are capable of making predictions given a dataset\n",
    "all predictors implements the **predict()** method that takes a dataset of new instances and returns a dataset of corresponding predictions\n",
    "they also have a **score()** method that measures the quality of the predictions given a test set\n",
    "all supervised estimators are also predictors\n",
    "i.e LinearRegression class \n",
    "\n",
    "steps:\n",
    "    1. choose a class of model\n",
    "    2. choose model hyperparameters\n",
    "    3. arrange data info feature matrix & target\n",
    "    4. fit(x_train,y_train) -> returned the model\n",
    "    5. transform(x_train) - for usually for unsupervised\n",
    "    6. transform(x_test) - if 5 needed \n",
    "    7. predict(x_test) -> returned the predicted labels\n",
    "\n",
    "@see linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in scikit-learn the hyper-parameters are passed as arguments to the constructor of the estimator\n",
    "to find the names and current values for all hyper parameters use **estimator.get_params()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers are usually combined with classifiers, regressors or other estimators to build a composite estimator\n",
    "Pipeline class can be used to chain multiple estimators into one\n",
    "This is useful as there is often a fixed sequence of steps in processing the data , for example feature selection, normalization and classification\n",
    "Pipeline serves multiple purposes:\n",
    "1. convenience and encapsulation - you only have to call fit() and predict() once on your data to fit the sequence of estimators\n",
    "2. joint parameter selection - you can grid search over parameters of all estimators in the pipeline at once\n",
    "3. safety - pipelines help avoid leaking statistics from your test data into the trained model in cross-validation , by ensuring that the same samples are used ti train the transformers and predictors\n",
    "\n",
    "example: \n",
    "pipeline = Pipeline([\n",
    "\n",
    "    ('imputer',SimpleImputer(strategy='median')),\n",
    "    \n",
    "    ('std_scalar',StandardScaler())\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit_transform() - > returned x_prepared\n",
    "\n",
    "pipeline.steps[i] - > access to individual estimator\n",
    "\n",
    "pipeline.named_steps['std_scalar'] -> access to individual estimator by its name\n",
    "\n",
    "pipeline.set_params(imputer\\__fill_value=10000) -> parameters of the estimator are accessed via \\estimator\\\\__\\parameter\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_pipeline utility\n",
    "takes a variable number of estimators and return a pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "make_pipeline(PCA(), SVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Column Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applies transformers to columns of an array or pandas DataFrame\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the results combined into a single feature space\n",
    "\n",
    "This is a particularly handy for datasets that contain heterogeneous data types, since we may want to scale the numeric features and one hot encode the categorial ones\n",
    "\n",
    "The order of the columns in the transformed feature matrix follows the order of how the columns are specified in the transformers list\n",
    "\n",
    "Columns of the original feature matrix that are not specified are dropped from the resulting transformed feature matrixm unless specified in the **passthrough** keyword\n",
    "\n",
    "Those columns specified with passthrough are added at the right to the output of the transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
